{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03940c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code for generating and running toy problems with pocomc sampler\n",
    "\"\"\"\n",
    "\n",
    "##################################################################################\n",
    "# 1. PACKAGES\n",
    "##################################################################################\n",
    "\n",
    "# diagnostics\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "import logging\n",
    "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
    "\n",
    "# Preconditioned Monte Carlo\n",
    "import numpy as np\n",
    "import pocomc as pc\n",
    "from scipy.stats import uniform\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.scipy.linalg import solve_triangular\n",
    "\n",
    "\n",
    "# my classes\n",
    "from likelihood import *\n",
    "from gaussian_mixture import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SUPPORTED_EXPERIMENTS = [\"gaussian\"]\n",
    "##################################################################################\n",
    "# 2. ARGUMENTS\n",
    "##################################################################################\n",
    "### The argparse is used to store and process any user input we want to pass on\n",
    "parser = argparse.ArgumentParser(description=\"Run experiment with specified parameters.\")\n",
    "parser.add_argument(\n",
    "    \"--experiment-type\",\n",
    "    choices=[\"gaussian\", \"dualmoon\", \"rosenbrock\"],\n",
    "    required=True,\n",
    "    help=\"Which experiment to run.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n-dims\",\n",
    "    type=int,\n",
    "    required=True,\n",
    "    help=\"Number of dimensions.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--outdir\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"The output directory, where things will be stored\"\n",
    ")\n",
    "\n",
    "# Everything below here are hyperparameters for the Gaussian experiment. \n",
    "parser.add_argument(\n",
    "    \"--nr-of-samples\",\n",
    "    type=int,\n",
    "    default=10000,\n",
    "    help=\"Number of samples to be geerated\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--nr-of-components\",\n",
    "    type=int,\n",
    "    default=2,\n",
    "    help=\"Number of components to be geerated\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--width-mean\",\n",
    "    type=float,\n",
    "    default=10.0,\n",
    "    help=\"The width of mean\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--width-cov\",\n",
    "    type=float,\n",
    "    default=3.0,\n",
    "    help=\"The width of cov\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--weights-of-components\",\n",
    "    nargs=\"+\",          \n",
    "    type=float,\n",
    "    default=None,\n",
    "    help=\"Mixture weights (--weights-of-components 0.3 0.7). If omitted, uses equal weights.\"\n",
    ")\n",
    "\n",
    "# Everything below here are hyperparameters for sampler\n",
    "parser.add_argument(\n",
    "    \"--prior-low\",\n",
    "    type=float,\n",
    "    default=-20.0,\n",
    "    help=\"Prior lower bound.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior-high\",\n",
    "    type=float,\n",
    "    default=20.0,\n",
    "    help=\"Prior upper bound.\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n-effective\",\n",
    "    type=int,\n",
    "    default=4096,\n",
    "    help=\"Effective number\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n-active\",\n",
    "    type=int,\n",
    "    default=2048,\n",
    "    help=\"Active number\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n-prior\",\n",
    "    type=int,\n",
    "    default=50000,\n",
    "    help=\"Active number\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# 3. EXPERIMENT RUNNER\n",
    "##################################################################################\n",
    "class SequentialMCExperimentRunner:\n",
    "    \"\"\"\n",
    "    Base class storing everything shared between different run experiments\n",
    "    \"\"\"\n",
    "    def __init__(self, args):\n",
    "        # Process the argparse args into params:\n",
    "        self.params = vars(args)\n",
    "\n",
    "        # Automatically create a unique output directory: results_1, results_2, ...\n",
    "        base_results_dir = self.params[\"outdir\"]\n",
    "        unique_outdir = self.get_next_available_outdir(base_results_dir)\n",
    "        print(f\"Using output directory: {unique_outdir}\")\n",
    "        os.makedirs(unique_outdir, exist_ok=False)\n",
    "        self.params[\"outdir\"] = unique_outdir\n",
    "\n",
    "        # Check if experiment type is allowed/supported:\n",
    "        if self.params[\"experiment_type\"] not in SUPPORTED_EXPERIMENTS:\n",
    "            raise ValueError(\n",
    "                f\"Experiment type {self.params['experiment_type']} is not supported. \"\n",
    "                f\"Supported types are: {SUPPORTED_EXPERIMENTS}\"\n",
    "            )\n",
    "\n",
    "        # Show the parameters to the screen/log file\n",
    "        print(\"Passed parameters:\")\n",
    "        for key, value in self.params.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "\n",
    "\n",
    "        # Specify the desired target function based on the experiment type\n",
    "        # ... your outdir logic etc ...\n",
    "        if self.params[\"experiment_type\"] == \"gaussian\":\n",
    "            print(\"Setting the target function to a standard Gaussian distribution.\")\n",
    "\n",
    "            # defining parameters for mcmc sampler / ground-truth generator\n",
    "            np.random.seed(900)\n",
    "\n",
    "            D = self.params[\"n_dims\"]\n",
    "            \n",
    "            true_samples, means, covariances, weights = GaussianMixtureGenerator.generate_gaussian_mixture(\n",
    "                n_dim=D,\n",
    "                n_gaussians=args.nr_of_components,\n",
    "                n_samples = args.nr_of_samples,\n",
    "                width_mean = args.width_mean,\n",
    "                width_cov = args.width_cov,\n",
    "                weights= args.weights_of_components,\n",
    "            ) \n",
    "\n",
    "            # Store true samples for diagnostics later on\n",
    "            self.true_samples = true_samples\n",
    "\n",
    "            # Convert to JAX arrays \n",
    "            self.mcmc_means   = jnp.stack(means, axis=0)         # (K, D)\n",
    "            self.mcmc_covs    = jnp.stack(covariances, axis=0)   # (K, D, D)\n",
    "            self.mcmc_weights = jnp.asarray(weights)             # (K,)\n",
    "\n",
    "            # Define Likelihood \n",
    "            self.likelihood = GaussianMixtureLikelihood(\n",
    "                means=self.mcmc_means,\n",
    "                covs=self.mcmc_covs,\n",
    "                weights=self.mcmc_weights,\n",
    "            )\n",
    "\n",
    "            self.target_fn = self.target_normal\n",
    "\n",
    "        \n",
    "\n",
    "    def get_next_available_outdir(self, base_dir: str, prefix: str = \"results\") -> str:\n",
    "        if not os.path.exists(base_dir):\n",
    "            os.makedirs(base_dir)\n",
    "\n",
    "        existing = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "        matches = [re.match(rf\"{prefix}_(\\d+)\", name) for name in existing]\n",
    "        numbers = [int(m.group(1)) for m in matches if m]\n",
    "        next_number = max(numbers, default=0) + 1\n",
    "        return os.path.join(base_dir, f\"{prefix}_{next_number}\")\n",
    "\n",
    "\n",
    "\n",
    "    def target_normal(self, x, data):\n",
    "        # x can be shape (D,) or (..., D); GaussianMixtureLikelihood.log_prob supports both\n",
    "        return self.likelihood.log_prob(x)\n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def make_auto_bounds_inflated(means, covs, inflate=9.0, nsig=12.0, pad=1e-6,\n",
    "                                    prior_low=None, prior_high=None):\n",
    "        \"\"\"\n",
    "        Per-dimension uniform bounds [low[d], high[d]] that \n",
    "        contain all mass of a Gaussian mixture:\n",
    "            - Bounds per component are wide enough to cover every single component;\n",
    "            - Created by means and cov and not \n",
    "        It is achieved by:\n",
    "            - find smallest and largest component means in dim k:\n",
    "            - for each component k, take the marginal sd in dim k \n",
    "            - set bounds\n",
    "        \"\"\"\n",
    "        means = np.asarray(means, dtype=float)           # (K, D)\n",
    "        covs  = np.asarray(covs,  dtype=float) * float(inflate)   # make variance bigger\n",
    "\n",
    "        # find smallest and largest component means in dim k\n",
    "        mu_min = means.min(axis=0)                       # (D,)\n",
    "        mu_max = means.max(axis=0)                       # (D,)\n",
    "\n",
    "        # for each component k, take the marginal sd in dim k \n",
    "        std_max = np.sqrt(np.stack([np.diag(C) for C in covs], axis=0)).max(axis=0)  # (D,)\n",
    "\n",
    "        # set bounds\n",
    "        low  = mu_min - nsig * std_max - pad\n",
    "        high = mu_max + nsig * std_max + pad\n",
    "\n",
    "        # if bounds are provided, make bigger bounds, if necessary\n",
    "        if prior_low  is not None: low  = np.minimum(low,  float(prior_low))\n",
    "        if prior_high is not None: high = np.maximum(high, float(prior_high))\n",
    "        return low, high\n",
    "    \n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    # 3.1. SAMPLER \n",
    "    #-----------------------------------------------------------------------------\n",
    "    def run_experiment(self):\n",
    "        \"\"\"\n",
    "        Run pocoMC sampler for the chosen experiment.\n",
    "        Stores results on self (similar to your MALA version) and returns a dict.\n",
    "        \"\"\"\n",
    "\n",
    "        dim = int(self.params[\"n_dims\"])\n",
    "        # \"data\" placeholder to preserve target_fn signature pattern\n",
    "        data = {}\n",
    "\n",
    "        # Prior: default to wide uniform [-30, 30] per dimension (override via params)\n",
    "        #prior_low  = float(self.params[\"prior_low\"])\n",
    "        #prior_high = float(self.params[\"prior_high\"])\n",
    "        #self.prior = pc.Prior(dim * [uniform(loc=prior_low, scale=prior_high - prior_low)])\n",
    "\n",
    "      \n",
    "        # Define our normal/Gaussian prior.\n",
    "        # prior = pc.Prior(dim*[norm(0.0, 3.0)]) # N(0,3)\n",
    "\n",
    "        # After you have self.mcmc_means / self.mcmc_covs available\n",
    "        means = np.asarray(self.mcmc_means)\n",
    "        covs  = np.asarray(self.mcmc_covs)\n",
    "\n",
    "        low, high = self.make_auto_bounds_inflated(\n",
    "            means, covs,\n",
    "            inflate=9.0,   # or smaller like 4.0 if you want tighter\n",
    "            nsig=12.0,     # conservative\n",
    "            prior_low=float(self.params[\"prior_low\"]),\n",
    "            prior_high=float(self.params[\"prior_high\"]),\n",
    "        )\n",
    "\n",
    "        self.prior = pc.Prior([\n",
    "            uniform(loc=low[d], scale=high[d] - low[d]) for d in range(dim)\n",
    "        ])\n",
    "\n",
    "        # Vectorized log-likelihood wrapper\n",
    "        # - pocoMC passes x as np.ndarray with shape (n_active, dim) when vectorize=True\n",
    "        # - must return np.ndarray shape (n_active,)\n",
    "        if hasattr(self, \"likelihood\"):\n",
    "            # GMM class: use JAX likelihood object's log_prob\n",
    "            like_obj = self.likelihood\n",
    "          \n",
    "            def log_likelihood(x, data):\n",
    "                out = self.target_normal(jnp.asarray(x), data)\n",
    "                out = np.asarray(out)                 # convert to numpy\n",
    "                out = np.atleast_1d(out).astype(np.float64)\n",
    "                return out\n",
    "\n",
    "        # -------------------------------------------------------------------------\n",
    "        # pocoMC sampler configuration (read from params with sensible defaults)\n",
    "        # -------------------------------------------------------------------------\n",
    "        # Early stopping and max steps (which is optional)\n",
    "        pc_n_steps = self.params.get(\"pc_n_steps\", None)\n",
    "        pc_n_max_steps = self.params.get(\"pc_n_max_steps\", None)\n",
    "        if pc_n_steps is not None:\n",
    "            pc_n_steps = int(pc_n_steps)\n",
    "        if pc_n_max_steps is not None:\n",
    "            pc_n_max_steps = int(pc_n_max_steps)\n",
    "\n",
    "        # Run control\n",
    "        n_total    = int(self.params.get(\"n_total\", 4096))\n",
    "        n_evidence = int(self.params.get(\"n_evidence\", 4096))\n",
    "        progress   = bool(self.params.get(\"progress\", True))\n",
    "        save_every = self.params.get(\"save_every\", None)\n",
    "        if save_every is not None:\n",
    "            save_every = int(save_every)\n",
    "\n",
    "        random_state = int(self.params.get(\"random_state\", 0))\n",
    "\n",
    "        # Save states into your experiment outdir (optional, but nice)\n",
    "        output_dir = self.params.get(\"outdir\", None)\n",
    "\n",
    "        sampler = pc.Sampler(\n",
    "            prior=self.prior,\n",
    "            likelihood=log_likelihood,\n",
    "            likelihood_args=[data],\n",
    "            vectorize=True,  # requires (n_active, n_dim)->(n_active,) :contentReference[oaicite:2]{index=2}\n",
    "            n_dim=dim,\n",
    "            n_effective=int(self.params.get(\"n_effective\")),\n",
    "            n_active=int(self.params.get(\"n_active\")),\n",
    "            #dynamic=True,\n",
    "            n_prior=int(self.params.get(\"n_prior\")),\n",
    "            precondition=bool(self.params.get(\"precondition\", True)),\n",
    "            flow= self.params.get(\"flow\", \"nsf6\"),\n",
    "            sample=self.params.get(\"sample_kernel\", \"tpcn\"),  # \"tpcn\" or \"rwm\" per docs :contentReference[oaicite:1]{index=1}\n",
    "            n_steps=pc_n_steps,\n",
    "            n_max_steps=pc_n_max_steps,\n",
    "            output_dir=output_dir,\n",
    "            output_label=\"pmc\",\n",
    "            random_state=random_state,\n",
    "        )\n",
    "\n",
    "        sampler.run(\n",
    "            n_total=n_total,\n",
    "            n_evidence=n_evidence,\n",
    "            progress=progress,\n",
    "            save_every=save_every,\n",
    "        )\n",
    "\n",
    "        # Posterior samples (resampled -> unweighted)\n",
    "        samples, logl, logp = sampler.posterior(resample=True)\n",
    "\n",
    "        # Evidence, it is optional\n",
    "        logZ, logZerr = sampler.evidence()\n",
    "\n",
    "        # Store results\n",
    "        self.samples = samples\n",
    "        self.logl = logl\n",
    "        self.logp = logp\n",
    "        self.logZ = logZ\n",
    "        self.logZerr = logZerr\n",
    "\n",
    "        self.results = {\n",
    "            \"samples\": samples,\n",
    "            \"logl\": logl,\n",
    "            \"logp\": logp,\n",
    "            \"logZ\": logZ,\n",
    "            \"logZerr\": logZerr,\n",
    "            \"sampler_results\": sampler.results,  # advanced per docs :contentReference[oaicite:3]{index=3}\n",
    "            \"params\": self.params,\n",
    "        }\n",
    "\n",
    "        print(\"Sampling complete!\")\n",
    "        print(f\"samples.shape = {samples.shape}\")\n",
    "        print(f\"logZ = {logZ} ± {logZerr}\")\n",
    "\n",
    "        return self.results\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    # 3.2. PLOT DIAGNOSTICS \n",
    "    #-----------------------------------------------------------------------------\n",
    "    def get_true_and_mcmc_samples(self, discard=0, thin=1):\n",
    "        dim = int(self.params[\"n_dims\"])\n",
    "\n",
    "        # True samples\n",
    "        if not hasattr(self, \"true_samples\") or self.true_samples is None:\n",
    "            raise ValueError(\"No true samples found. Ensure self.true_samples is set (gaussian experiment).\")\n",
    "\n",
    "        true_np = np.asarray(self.true_samples).reshape(-1, dim)\n",
    "\n",
    "        # Sampler samples\n",
    "        if hasattr(self, \"samples\") and self.samples is not None:\n",
    "            # pocoMC: self.samples is already (N, dim)\n",
    "            samp = np.asarray(self.samples).reshape(-1, dim)\n",
    "            samp = samp[int(discard)::int(thin), :]\n",
    "            mcmc_np = samp\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"No sampler samples found. Run run_experiment() first. \"\n",
    "                \"Expected either self.samples (pocoMC) or self.chains (BlackJAX).\"\n",
    "            )\n",
    "\n",
    "        return true_np, mcmc_np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def plot_true_vs_mcmc_corner(self, seed=2046):\n",
    "        \"\"\"\n",
    "        Overlay corner plot:\n",
    "        - MCMC production samples (black)\n",
    "        - true samples (red)\n",
    "        Saves: true_vs_mcmc_corner_plot.pdf\n",
    "        \"\"\"\n",
    "        # Get samples \n",
    "        true_np, mcmc_np = self.get_true_and_mcmc_samples()\n",
    "\n",
    "        dim = int(self.params[\"n_dims\"])\n",
    "        labels = [f\"x{i}\" for i in range(dim)]\n",
    "\n",
    "        outdir = self.params[\"outdir\"]\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        # Plot MCMC first \n",
    "        fig = corner.corner(\n",
    "            mcmc_np,\n",
    "            color=\"black\",\n",
    "            hist_kwargs={\"color\": \"black\", \"density\": True},\n",
    "            show_titles=True,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        # Overlay true samples \n",
    "        corner.corner(\n",
    "            true_np,\n",
    "            fig=fig,\n",
    "            color=\"red\",\n",
    "            hist_kwargs={\"color\": \"red\", \"density\": True},\n",
    "            show_titles=True,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        # Legend\n",
    "        handles = [\n",
    "            plt.Line2D([], [], color=\"black\", label=\"pocomc\"),\n",
    "            plt.Line2D([], [], color=\"red\", label=\"True Normal\"),\n",
    "        ]\n",
    "        fig.legend(handles=handles, loc=\"upper right\")\n",
    "\n",
    "        save_name = os.path.join(outdir, \"true_vs_mcmc_corner_plot.pdf\")\n",
    "        fig.savefig(save_name, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved overlay corner plot to {save_name}\")\n",
    "\n",
    "\n",
    "\n",
    "    def plot_acceptance_rate(self):\n",
    "        print(\"Plotting acceptance-rate diagnostic curve...\")\n",
    "\n",
    "        if hasattr(self, \"sampler\") and self.sampler is not None:\n",
    "            results = self.sampler.results\n",
    "        else:\n",
    "            results = self.results[\"sampler_results\"]\n",
    "\n",
    "        accept = np.asarray(results[\"accept\"]).reshape(-1)\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(accept)\n",
    "        plt.xlabel(\"PMC/SMC Iteration\")\n",
    "        plt.ylabel(\"Acceptance rate\")\n",
    "        plt.title(\"pocoMC Acceptance Rate\")\n",
    "\n",
    "        save_name = os.path.join(self.params[\"outdir\"], \"acceptance_rate_curve.pdf\")\n",
    "        plt.savefig(save_name, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"Saved to {save_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    # 3.3. SAMPLE STATISTICS\n",
    "    #-----------------------------------------------------------------------------\n",
    "    def save_samples_json(self):\n",
    "        # output directory \n",
    "        outdir = self.params[\"outdir\"]\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        # get samples once\n",
    "        true_np, mcmc_np = self.get_true_and_mcmc_samples()\n",
    "\n",
    "        # save generated samples\n",
    "        mcmc_path = os.path.join(outdir, \"mcmc_samples.json\")\n",
    "        with open(mcmc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(mcmc_np.tolist(), f)\n",
    "        print(f\"MCMC samples saved to {mcmc_path}\")\n",
    "\n",
    "        # save true samples\n",
    "        true_path = os.path.join(outdir, \"true_samples.json\")\n",
    "        with open(true_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(true_np.tolist(), f)\n",
    "        print(f\"True samples saved to {true_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_and_save_sample_statistics(self):\n",
    "        \"\"\"\n",
    "        Computes and saves per-dimension statistics for:\n",
    "        - MCMC production samples\n",
    "        - true samples\n",
    "        Saves: sample_statistics.txt in self.params[\"outdir\"]\n",
    "        \"\"\"\n",
    "\n",
    "        # get samples \n",
    "        true_samples, mcmc_samples = self.get_true_and_mcmc_samples()\n",
    "\n",
    "        # MCMC stats\n",
    "        self.pm = mcmc_samples.mean(axis=0)\n",
    "        self.pv = mcmc_samples.var(axis=0)\n",
    "        self.ps = mcmc_samples.std(axis=0)\n",
    "\n",
    "        # True stats\n",
    "        self.qm = true_samples.mean(axis=0)\n",
    "        self.qv = true_samples.var(axis=0)\n",
    "        self.qs = true_samples.std(axis=0)\n",
    "\n",
    "        # store arrays \n",
    "        self.mcmc_samples = mcmc_samples\n",
    "        self.true_samples_np = true_samples\n",
    "\n",
    "        np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "        stats_str = (\n",
    "            \"pm (mean of MCMC samples):\\n\" + str(self.pm) +\n",
    "            \"\\n\\npv (variance of MCMC samples):\\n\" + str(self.pv) +\n",
    "            \"\\n\\nps (std dev of MCMC samples):\\n\" + str(self.ps) +\n",
    "            \"\\n\\nqm (mean of true samples):\\n\" + str(self.qm) +\n",
    "            \"\\n\\nqv (variance of true samples):\\n\" + str(self.qv) +\n",
    "            \"\\n\\nqs (std dev of true samples):\\n\" + str(self.qs) + \"\\n\"\n",
    "        )\n",
    "\n",
    "        outdir = self.params[\"outdir\"]\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        stats_path = os.path.join(outdir, \"sample_statistics.txt\")\n",
    "        with open(stats_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(stats_str)\n",
    "\n",
    "        print(f\"Sample statistics saved to {stats_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    # 3.4. KL DIVERGENCE\n",
    "    #-----------------------------------------------------------------------------\n",
    "    import numpy as np, warnings, os\n",
    "    from typing import Tuple\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def gau_kl(pm: np.ndarray, pv: np.ndarray,\n",
    "               qm: np.ndarray, qv: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Kullback-Liebler divergence from Gaussian pm,pv to Gaussian qm,qv.\n",
    "        Also computes KL divergence from a single Gaussian pm,pv to a set\n",
    "         of Gaussians qm,qv.\n",
    "        Diagonal covariances are assumed.  Divergence is expressed in nats.\n",
    "        \"\"\"\n",
    "        if (len(qm.shape) == 2):\n",
    "            axis = 1\n",
    "        else:\n",
    "            axis = 0\n",
    "        # Determinants of diagonal covariances pv, qv\n",
    "        dpv = pv.prod()\n",
    "        dqv = qv.prod(axis)\n",
    "        # Inverse of diagonal covariance qv\n",
    "        iqv = 1. / qv\n",
    "        # Difference between means pm, qm\n",
    "        diff = qm - pm\n",
    "        return (0.5 * (\n",
    "            np.log(dqv / dpv)                 # log |\\Sigma_q| / |\\Sigma_p|\n",
    "            + (iqv * pv).sum(axis)            # + tr(\\Sigma_q^{-1} * \\Sigma_p)\n",
    "            + (diff * iqv * diff).sum(axis)   # + (\\mu_q-\\mu_p)^T\\Sigma_q^{-1}(\\mu_q-\\mu_p)\n",
    "            - len(pm)                         # - N\n",
    "        ))\n",
    "    \n",
    "\n",
    "    def kl_metrics(\n",
    "        self,\n",
    "        outdir: str | None = None,\n",
    "        filename: str = \"kl_metrics.txt\",\n",
    "    ) -> None:\n",
    "        import os\n",
    "        import numpy as np\n",
    "\n",
    "        # define outdir\n",
    "        outdir = (\n",
    "            outdir\n",
    "            or (getattr(self, \"params\", {}) or {}).get(\"outdir\", None)\n",
    "            or getattr(self, \"outdir\", None)\n",
    "        )\n",
    "        if outdir is None:\n",
    "            raise ValueError(\"No output directory specified (pass outdir=... or set params['outdir']).\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        \n",
    "        true_np, mcmc_np = self.get_true_and_mcmc_samples() \n",
    "\n",
    "        # Parametric Gaussian stats (diagonal covariance assumed)\n",
    "        pm = mcmc_np.mean(axis=0)\n",
    "        pv = mcmc_np.var(axis=0)\n",
    "        qm = true_np.mean(axis=0)\n",
    "        qv = true_np.var(axis=0)\n",
    "\n",
    "        kl_val = self.gau_kl(pm, pv, qm, qv)  # scalar for 1D qm/qv\n",
    "\n",
    "        out_path = os.path.join(outdir, filename)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            if np.isscalar(kl_val):\n",
    "                f.write(f\"Parametric KL (Gaussian): {float(kl_val):.8f}\\n\")\n",
    "            else:\n",
    "                kl_arr = np.asarray(kl_val).ravel()\n",
    "                f.write(\"Parametric KL (Gaussian):\\n\")\n",
    "                for i, v in enumerate(kl_arr):\n",
    "                    f.write(f\"  [{i}] {float(v):.8f}\\n\")\n",
    "\n",
    "        print(f\"KL metrics saved to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef8342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using output directory: ./runs/gaussian_2d\\results_13\n",
      "Passed parameters:\n",
      "experiment_type: gaussian\n",
      "n_dims: 10\n",
      "outdir: ./runs/gaussian_2d\\results_13\n",
      "nr_of_samples: 10000\n",
      "nr_of_components: 4\n",
      "width_mean: 10.0\n",
      "width_cov: 1.0\n",
      "weights_of_components: [0.25, 0.25, 0.25, 0.25]\n",
      "prior_low: -25.0\n",
      "prior_high: 25.0\n",
      "n_effective: 16000\n",
      "n_active: 8000\n",
      "n_prior: 150000\n",
      "Setting the target function to a standard Gaussian distribution.\n",
      "[-1.9527 -5.8704  7.5673 -5.8053 -0.1505 -0.7373 -3.4366  5.8232  1.8345\n",
      " -1.4392]\n",
      "[-1.7255  1.3081  2.4974 -0.9036 -6.6159  3.2771  6.3532 -2.7347  7.1564\n",
      "  3.3161]\n",
      "[-1.6282 -0.0014  3.6579 -2.1062 -0.2691  4.459   9.6636  8.9419 -2.9114\n",
      " -5.4834]\n",
      "[ 1.3373 -3.7496  6.6619  6.7686  7.7773 -0.3607 -8.9235 -7.398   7.5108\n",
      " -6.5496]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iter: 52it [1:07:27, 77.84s/it, beta=1, calls=1740096, ESS=18044, logZ=-51.4, logP=-68, acc=0.82, steps=5, eff=1]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling complete!\n",
      "samples.shape = (44561, 10)\n",
      "logZ = -51.59464312615643 ± 0.008917035592940308\n",
      "Saved overlay corner plot to ./runs/gaussian_2d\\results_13\\true_vs_mcmc_corner_plot.pdf\n",
      "Plotting acceptance-rate diagnostic curve...\n",
      "Saved to ./runs/gaussian_2d\\results_13\\acceptance_rate_curve.pdf\n",
      "MCMC samples saved to ./runs/gaussian_2d\\results_13\\mcmc_samples.json\n",
      "True samples saved to ./runs/gaussian_2d\\results_13\\true_samples.json\n",
      "Sample statistics saved to ./runs/gaussian_2d\\results_13\\sample_statistics.txt\n",
      "KL metrics saved to ./runs/gaussian_2d\\results_13\\kl_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    ### The argparse is used to store and process any user input we want to pass on\n",
    "    \"notebook\",\n",
    "    \"--experiment-type\", \"gaussian\",\n",
    "    \"--outdir\", \"./runs/gaussian_2d\",\n",
    "\n",
    "    # Everything below here are hyperparameters for the gaussians.\n",
    "    \"--n-dims\", \"8\",\n",
    "    \"--nr-of-samples\", \"10000\",\n",
    "    \"--nr-of-components\", \"4\",\n",
    "    \"--width-mean\", \"10.0\",\n",
    "    \"--width-cov\", \"1.0\",\n",
    "    \"--weights-of-components\", \"0.25\", \"0.25\", \"0.25\", \"0.25\", \n",
    "\n",
    "    # Everything below here are hyperparameters for the samplers.\n",
    "    \"--prior-low\", \"-25.0\",\n",
    "    \"--prior-high\", \"25.0\",\n",
    "    \"--n-effective\", \"16000\",\n",
    "    \"--n-active\", \"8000\",\n",
    "    \"--n-prior\", \"150000\"\n",
    "]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get the arguments passed over from the command line, and create the experiment runner\n",
    "    args = parser.parse_args()\n",
    "    runner = SequentialMCExperimentRunner(args)\n",
    "    runner.run_experiment()\n",
    "    runner.plot_true_vs_mcmc_corner()\n",
    "    runner.plot_acceptance_rate()\n",
    "    runner.save_samples_json()\n",
    "    runner.compute_and_save_sample_statistics()\n",
    "    runner.kl_metrics()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRASP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
