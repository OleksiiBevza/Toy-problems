{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f8010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Callable, Mapping, Tuple, Any, Optional, Dict\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from scaler_jax import inverse_jax, forward_jax, apply_boundary_conditions_x_jax\n",
    "\n",
    "\n",
    "Array = jax.Array\n",
    "\n",
    "\n",
    "def _flow_u_to_theta(flow, u: Array, condition: Optional[Array] = None) -> Tuple[Array, Array]:\n",
    "    \"\"\"\n",
    "        * mapping u into theta using FlowJAX bijection\n",
    "        * return log|det du/dtheta|.\n",
    "\n",
    "    FlowJAX:\n",
    "        * transform_and_log_det(u) returns (theta, log|det dtheta/du|)\n",
    "        * get log|det du/dtheta|.\n",
    "    \"\"\"\n",
    "    theta, fwd_logdet = flow.bijection.transform_and_log_det(u, condition)\n",
    "    return theta, -fwd_logdet\n",
    "\n",
    "\n",
    "def _flow_theta_to_u(flow, theta: Array, condition: Optional[Array] = None) -> Tuple[Array, Array]:\n",
    "    \"\"\"\n",
    "        * mapping theta into u using FlowJAX bijection\n",
    "        * return log|det du/dtheta| directly.\n",
    "    \"\"\"\n",
    "    u, inv_logdet = flow.bijection.inverse_and_log_det(theta, condition)\n",
    "    return u, inv_logdet\n",
    "\n",
    "\n",
    "def preconditioned_pcn_jax(\n",
    "    key: Array,\n",
    "    *,\n",
    "    # --- current state (all arrays; no None) ---\n",
    "    u: Array,                 # (N, D)\n",
    "    x: Array,                 # (N, D)\n",
    "    logdetj: Array,           # (N,)\n",
    "    logl: Array,              # (N,)\n",
    "    logp: Array,              # (N,)\n",
    "    logdetj_flow: Array,      # (N,)\n",
    "    blobs: Array,             # (N, B...) ; use shape (N, 0) if no blobs\n",
    "\n",
    "    beta: Array,              # scalar\n",
    "\n",
    "    # --- functions ---\n",
    "    loglike_fn: Callable[[Array], Tuple[Array, Array]],\n",
    "    logprior_fn: Callable[[Array], Array],\n",
    "    flow: Any,                # FlowJAX Transformed-like object with .bijection\n",
    "    scaler_cfg: Mapping[str, Array],\n",
    "    scaler_masks: Mapping[str, Array],\n",
    "\n",
    "    # --- geometry (Student-t) ---\n",
    "    geom_mu: Array,           # (D,)\n",
    "    geom_cov: Array,          # (D, D)\n",
    "    geom_nu: Array,           # scalar\n",
    "\n",
    "    # --- options ---\n",
    "    n_max: int,\n",
    "    n_steps: int,\n",
    "    proposal_scale: Array,    # scalar\n",
    "    condition: Optional[Array] = None,\n",
    ") -> Dict[str, Array]:\n",
    "    \"\"\"\n",
    "    Doubly Preconditioned Crankâ€“Nicolson (PCN), JAX version.\n",
    "\n",
    "    Requirements:\n",
    "      - logprior_fn(x_i): x_i has shape (D,), returns scalar\n",
    "      - loglike_fn(x_i): x_i has shape (D,), returns (scalar_loglike, blob_i)\n",
    "        where blob_i has fixed shape (B...) matching blobs[0].\n",
    "\n",
    "    Info:\n",
    "      - All randomness uses `key` and returns updated `key` (make sure it is pure and check randomness).\n",
    "      - FlowJAX bijections are vmapped across walkers.\n",
    "    \"\"\"\n",
    "    u = jnp.asarray(u)\n",
    "    x = jnp.asarray(x)\n",
    "    logdetj = jnp.asarray(logdetj)\n",
    "    logl = jnp.asarray(logl)\n",
    "    logp = jnp.asarray(logp)\n",
    "    logdetj_flow = jnp.asarray(logdetj_flow)\n",
    "    blobs = jnp.asarray(blobs)\n",
    "    beta = jnp.asarray(beta)\n",
    "    proposal_scale = jnp.asarray(proposal_scale)\n",
    "\n",
    "    geom_mu = jnp.asarray(geom_mu)\n",
    "    geom_cov = jnp.asarray(geom_cov)\n",
    "    geom_nu = jnp.asarray(geom_nu)\n",
    "\n",
    "    n_walkers, n_dim = u.shape\n",
    "\n",
    "    inv_cov = jnp.linalg.inv(geom_cov)\n",
    "    chol_cov = jnp.linalg.cholesky(geom_cov)\n",
    "\n",
    "    # --- Flow: u -> theta (batched via vmap) ---\n",
    "    def _u2t_single(ui: Array) -> Tuple[Array, Array]:\n",
    "        return _flow_u_to_theta(flow, ui, condition)\n",
    "\n",
    "    theta, logdetj_flow0 = jax.vmap(_u2t_single, in_axes=0, out_axes=(0, 0))(u)\n",
    "\n",
    "    # initial mean and counter and objective\n",
    "    mu = geom_mu\n",
    "    sigma0 = jnp.minimum(proposal_scale, jnp.asarray(0.99, dtype=u.dtype))\n",
    "    logp2_best = jnp.mean(logl + logp)\n",
    "    cnt0 = jnp.asarray(0, dtype=jnp.int32)\n",
    "    i0 = jnp.asarray(0, dtype=jnp.int32)\n",
    "    calls0 = jnp.asarray(0, dtype=jnp.int32)\n",
    "    accept0 = jnp.asarray(0.0, dtype=u.dtype)\n",
    "    done0 = jnp.asarray(False)\n",
    "\n",
    "    # update initial flow logdet with computed one \n",
    "    logdetj_flow = logdetj_flow0\n",
    "\n",
    "    blob_template = jnp.zeros_like(blobs[0])\n",
    "\n",
    "    # helpers: Student-t form\n",
    "    def _quad(diff_: Array) -> Array:\n",
    "        tmp = diff_ @ inv_cov\n",
    "        return jnp.sum(tmp * diff_, axis=1)\n",
    "\n",
    "    # --- skip invalid walkers ---\n",
    "    def _prior_or_neginf(xi: Array, ok: Array) -> Array:\n",
    "        return jax.lax.cond(\n",
    "            ok,\n",
    "            lambda z: logprior_fn(z),\n",
    "            lambda z: jnp.asarray(-jnp.inf, dtype=xi.dtype),\n",
    "            xi,\n",
    "        )\n",
    "\n",
    "    def _like_or_neginf(xi: Array, ok: Array) -> Tuple[Array, Array]:\n",
    "        def _do(z: Array) -> Tuple[Array, Array]:\n",
    "            ll, bb = loglike_fn(z)\n",
    "            return ll, bb\n",
    "\n",
    "        def _skip(z: Array) -> Tuple[Array, Array]:\n",
    "            return jnp.asarray(-jnp.inf, dtype=xi.dtype), blob_template\n",
    "\n",
    "        return jax.lax.cond(ok, _do, _skip, xi)\n",
    "\n",
    "    \n",
    "    # (key, u, x, theta, logdetj, logdetj_flow, logl, logp, blobs, mu, sigma, logp2_best, cnt, i, calls, accept, done)\n",
    "    carry0 = (\n",
    "        key, u, x, theta, logdetj, logdetj_flow, logl, logp, blobs,\n",
    "        mu, sigma0, logp2_best, cnt0, i0, calls0, accept0, done0\n",
    "    )\n",
    "\n",
    "    max_sigma_cap = jnp.minimum(jnp.asarray(2.38, dtype=u.dtype) / jnp.sqrt(jnp.asarray(n_dim, dtype=u.dtype)),\n",
    "                                jnp.asarray(0.99, dtype=u.dtype))\n",
    "\n",
    "    def cond_fn(carry):\n",
    "        (_, _, _, _, _, _, _, _, _, _, _, _, _, i, _, _, done) = carry\n",
    "        return (i < jnp.asarray(n_max, dtype=i.dtype)) & (~done)\n",
    "\n",
    "    def body_fn(carry):\n",
    "        (key, u, x, theta, logdetj, logdetj_flow, logl, logp, blobs,\n",
    "         mu, sigma, logp2_best, cnt, i, calls, accept, done) = carry\n",
    "\n",
    "        i1 = i + jnp.asarray(1, dtype=i.dtype)\n",
    "\n",
    "        key, k_gamma, k_norm, k_unif = jax.random.split(key, 4)\n",
    "\n",
    "        diff = theta - mu\n",
    "        quad = _quad(diff)\n",
    "\n",
    "        a = (jnp.asarray(n_dim, dtype=u.dtype) + geom_nu) / jnp.asarray(2.0, dtype=u.dtype)\n",
    "        z = jax.random.gamma(k_gamma, a, shape=(n_walkers,))  # unit scale\n",
    "        s = (geom_nu + quad) / (jnp.asarray(2.0, dtype=u.dtype) * z)\n",
    "\n",
    "        eps = jax.random.normal(k_norm, shape=(n_walkers, n_dim), dtype=u.dtype)\n",
    "        noise = eps @ chol_cov.T\n",
    "\n",
    "        theta_prime = (\n",
    "            mu\n",
    "            + jnp.sqrt(jnp.asarray(1.0, dtype=u.dtype) - sigma * sigma) * diff\n",
    "            + sigma * jnp.sqrt(s)[:, None] * noise\n",
    "        )\n",
    "\n",
    "        # --- Flow: theta into u (batched via vmap) ---\n",
    "        def _t2u_single(ti: Array) -> Tuple[Array, Array]:\n",
    "            return _flow_theta_to_u(flow, ti, condition)\n",
    "\n",
    "        u_prime, logdetj_flow_prime = jax.vmap(_t2u_single, in_axes=0, out_axes=(0, 0))(theta_prime)\n",
    "\n",
    "        # --- Scaler inverse: u into x, ---\n",
    "        #TODO check boundary handling here \n",
    "        x_prime, logdetj_prime = inverse_jax(u_prime, scaler_cfg, scaler_masks)\n",
    "\n",
    "        x_prime_bc = apply_boundary_conditions_x_jax(x_prime, dict(scaler_cfg))\n",
    "        u_prime_bc = forward_jax(x_prime_bc, scaler_cfg, scaler_masks)\n",
    "        x_prime, logdetj_prime = inverse_jax(u_prime_bc, scaler_cfg, scaler_masks)\n",
    "\n",
    "        u_prime = u_prime_bc\n",
    "\n",
    "        finite0 = jnp.isfinite(logdetj_prime) & jnp.all(jnp.isfinite(x_prime), axis=1)\n",
    "\n",
    "        logp_prime = jax.vmap(_prior_or_neginf, in_axes=(0, 0), out_axes=0)(x_prime, finite0)\n",
    "        finite1 = finite0 & jnp.isfinite(logp_prime)\n",
    "\n",
    "        logl_prime, blobs_prime = jax.vmap(_like_or_neginf, in_axes=(0, 0), out_axes=(0, 0))(x_prime, finite1)\n",
    "\n",
    "        calls = calls + jnp.sum(finite1.astype(jnp.int32))\n",
    "\n",
    "        diff_prime = theta_prime - mu\n",
    "        quad_prime = _quad(diff_prime)\n",
    "\n",
    "        coef = -(jnp.asarray(n_dim, dtype=u.dtype) + geom_nu) / jnp.asarray(2.0, dtype=u.dtype)\n",
    "        A = coef * jnp.log1p(quad_prime / geom_nu)\n",
    "        B = coef * jnp.log1p(quad / geom_nu)\n",
    "\n",
    "        log_alpha = (\n",
    "            (logl_prime - logl) * beta\n",
    "            + (logp_prime - logp)\n",
    "            + (logdetj_prime - logdetj)\n",
    "            + (logdetj_flow_prime - logdetj_flow)\n",
    "            - A + B\n",
    "        )\n",
    "\n",
    "        alpha = jnp.exp(jnp.minimum(jnp.asarray(0.0, dtype=u.dtype), log_alpha))\n",
    "        alpha = jnp.where(jnp.isnan(alpha), jnp.asarray(0.0, dtype=u.dtype), alpha)\n",
    "\n",
    "        u_rand = jax.random.uniform(k_unif, shape=(n_walkers,), dtype=u.dtype)\n",
    "        accept_mask = u_rand < alpha\n",
    "\n",
    "        # accept / reject\n",
    "        # TODO check this\n",
    "        theta = jnp.where(accept_mask[:, None], theta_prime, theta)\n",
    "        u = jnp.where(accept_mask[:, None], u_prime, u)\n",
    "        x = jnp.where(accept_mask[:, None], x_prime, x)\n",
    "\n",
    "        logdetj = jnp.where(accept_mask, logdetj_prime, logdetj)\n",
    "        logdetj_flow = jnp.where(accept_mask, logdetj_flow_prime, logdetj_flow)\n",
    "        logl = jnp.where(accept_mask, logl_prime, logl)\n",
    "        logp = jnp.where(accept_mask, logp_prime, logp)\n",
    "        blobs = jnp.where(accept_mask.reshape((n_walkers,) + (1,) * (blobs.ndim - 1)), blobs_prime, blobs)\n",
    "\n",
    "        accept = jnp.mean(alpha)\n",
    "\n",
    "        # TODO check\n",
    "        step = jnp.asarray(1.0, dtype=u.dtype) / jnp.power(jnp.asarray(i1 + 1, dtype=u.dtype), jnp.asarray(0.75, dtype=u.dtype))\n",
    "        sigma = sigma + step * (accept - jnp.asarray(0.234, dtype=u.dtype))\n",
    "        sigma = jnp.abs(jnp.minimum(sigma, max_sigma_cap))\n",
    "\n",
    "        mu_step = jnp.asarray(1.0, dtype=u.dtype) / jnp.asarray(i1 + 1, dtype=u.dtype)\n",
    "        mu = mu + mu_step * (jnp.mean(theta, axis=0) - mu)\n",
    "\n",
    "        logp2_new = jnp.mean(logl + logp)\n",
    "        improved = logp2_new > logp2_best\n",
    "        cnt = jnp.where(improved, jnp.asarray(0, dtype=cnt.dtype), cnt + jnp.asarray(1, dtype=cnt.dtype))\n",
    "        logp2_best = jnp.where(improved, logp2_new, logp2_best)\n",
    "\n",
    "        thresh = jnp.asarray(n_steps, dtype=u.dtype) * jnp.power(\n",
    "            (jnp.asarray(2.38, dtype=u.dtype) / jnp.sqrt(jnp.asarray(n_dim, dtype=u.dtype))) / sigma,\n",
    "            jnp.asarray(2.0, dtype=u.dtype),\n",
    "        )\n",
    "        done = cnt.astype(u.dtype) >= thresh\n",
    "\n",
    "        return (\n",
    "            key, u, x, theta, logdetj, logdetj_flow, logl, logp, blobs,\n",
    "            mu, sigma, logp2_best, cnt, i1, calls, accept, done\n",
    "        )\n",
    "\n",
    "    carry_f = jax.lax.while_loop(cond_fn, body_fn, carry0)\n",
    "\n",
    "    (key, u, x, theta, logdetj, logdetj_flow, logl, logp, blobs,\n",
    "     mu, sigma, logp2_best, cnt, i, calls, accept, done) = carry_f\n",
    "\n",
    "    return {\n",
    "        \"key\": key,\n",
    "        \"u\": u,\n",
    "        \"x\": x,\n",
    "        \"logdetj\": logdetj,\n",
    "        \"logdetj_flow\": logdetj_flow,\n",
    "        \"logl\": logl,\n",
    "        \"logp\": logp,\n",
    "        \"blobs\": blobs,\n",
    "        \"efficiency\": sigma,\n",
    "        \"accept\": accept,\n",
    "        \"steps\": i,\n",
    "        \"calls\": calls,\n",
    "        \"proposal_scale\": sigma,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb4144d",
   "metadata": {},
   "source": [
    "---\n",
    "# test\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc3fb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# EXPERIMENT RUNNER (UPDATED FOR preconditioned_pcn_jax)\n",
    "##################################################################################\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from likelihood import *\n",
    "from gaussian_mixture import *\n",
    "import logging\n",
    "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "\n",
    "SUPPORTED_EXPERIMENTS = {\"gaussian\"}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def mixture_mean_cov(means: jnp.ndarray, covs: jnp.ndarray, weights: jnp.ndarray, jitter: float = 1e-6):\n",
    "    \"\"\"\n",
    "    Compute mixture mean/cov for a Gaussian mixture:\n",
    "      means:   (K, D)\n",
    "      covs:    (K, D, D)\n",
    "      weights: (K,)\n",
    "    Returns:\n",
    "      mu:  (D,)\n",
    "      cov: (D, D)\n",
    "    \"\"\"\n",
    "    w = weights / jnp.sum(weights)\n",
    "    mu = jnp.sum(w[:, None] * means, axis=0)  # (D,)\n",
    "\n",
    "    diff = means - mu[None, :]  # (K, D)\n",
    "    outer = diff[:, :, None] * diff[:, None, :]  # (K, D, D)\n",
    "\n",
    "    cov = jnp.sum(w[:, None, None] * (covs + outer), axis=0)\n",
    "    cov = cov + jitter * jnp.eye(cov.shape[0], dtype=cov.dtype)\n",
    "    return mu, cov\n",
    "\n",
    "\n",
    "def make_uniform_box_logprior(low: jnp.ndarray, high: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Uniform prior on a hyper-rectangle:\n",
    "      log p(x) = 0 inside [low, high], -inf outside\n",
    "    \"\"\"\n",
    "    low = jnp.asarray(low)\n",
    "    high = jnp.asarray(high)\n",
    "\n",
    "    def logprior_fn(x: jnp.ndarray) -> jnp.ndarray:\n",
    "        inside = jnp.all((x >= low) & (x <= high))\n",
    "        return jax.lax.select(\n",
    "            inside,\n",
    "            jnp.asarray(0.0, dtype=x.dtype),\n",
    "            jnp.asarray(-jnp.inf, dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "    return logprior_fn\n",
    "\n",
    "\n",
    "class IdentityBijection:\n",
    "    def transform_and_log_det(self, u, condition=None):\n",
    "        return u, jnp.asarray(0.0, dtype=u.dtype)\n",
    "\n",
    "    def inverse_and_log_det(self, theta, condition=None):\n",
    "        return theta, jnp.asarray(0.0, dtype=theta.dtype)\n",
    "\n",
    "\n",
    "class IdentityFlow:\n",
    "    \"\"\"Use this only to validate wiring when you don't have a trained FlowJAX flow yet.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.bijection = IdentityBijection()\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# Runner\n",
    "##################################################################################\n",
    "class pcn_ExperimentRunner:\n",
    "    \"\"\"\n",
    "    Runner that can generate samples using preconditioned_pcn_jax.\n",
    "\n",
    "    Key design decisions:\n",
    "      - We run the kernel multiple times (n_outer), each time updating the full ensemble state.\n",
    "      - We store x across outer iterations -> samples shape (n_outer, N_walkers, D).\n",
    "        Your diagnostics can flatten via reshape(-1, D).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, *, flow=None, scaler_cfg=None, scaler_masks=None):\n",
    "        self.params = vars(args)\n",
    "\n",
    "        # --- unique outdir ---\n",
    "        base_results_dir = self.params[\"outdir\"]\n",
    "        unique_outdir = self.get_next_available_outdir(base_results_dir)\n",
    "        print(f\"Using output directory: {unique_outdir}\")\n",
    "        os.makedirs(unique_outdir, exist_ok=False)\n",
    "        self.params[\"outdir\"] = unique_outdir\n",
    "\n",
    "        # --- validate experiment ---\n",
    "        if self.params[\"experiment_type\"] not in SUPPORTED_EXPERIMENTS:\n",
    "            raise ValueError(\n",
    "                f\"Experiment type {self.params['experiment_type']} is not supported. \"\n",
    "                f\"Supported types are: {SUPPORTED_EXPERIMENTS}\"\n",
    "            )\n",
    "\n",
    "        print(\"Passed parameters:\")\n",
    "        for k, v in self.params.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "        # Attach (or later set) flow + scaler\n",
    "        self.flow = flow\n",
    "        self.scaler_cfg = scaler_cfg\n",
    "        self.scaler_masks = scaler_masks\n",
    "\n",
    "        # Setup experiment\n",
    "        if self.params[\"experiment_type\"] == \"gaussian\":\n",
    "            self._setup_gaussian_experiment(args)\n",
    "\n",
    "        # Placeholder for results\n",
    "        self.samples = None\n",
    "        self.accept_history = None\n",
    "        self.sigma_history = None\n",
    "        self.calls_history = None\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Setup\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _setup_gaussian_experiment(self, args):\n",
    "        print(\"Setting the target function to a Gaussian mixture distribution.\")\n",
    "\n",
    "        np.random.seed(900)\n",
    "\n",
    "        D = int(self.params[\"n_dims\"])\n",
    "\n",
    "        # Generate \"true\" samples and mixture parameters (your existing generator)\n",
    "        true_samples, means, covariances, weights = GaussianMixtureGenerator.generate_gaussian_mixture(\n",
    "            n_dim=D,\n",
    "            n_gaussians=args.nr_of_components,\n",
    "            n_samples=args.nr_of_samples,\n",
    "            width_mean=args.width_mean,\n",
    "            width_cov=args.width_cov,\n",
    "            weights=args.weights_of_components,\n",
    "        )\n",
    "\n",
    "        self.true_samples = true_samples\n",
    "\n",
    "        # Convert mixture params to JAX arrays\n",
    "        self.mcmc_means = jnp.stack(means, axis=0)        # (K, D)\n",
    "        self.mcmc_covs = jnp.stack(covariances, axis=0)   # (K, D, D)\n",
    "        self.mcmc_weights = jnp.asarray(weights)          # (K,)\n",
    "\n",
    "        # Likelihood object you already have\n",
    "        self.likelihood = GaussianMixtureLikelihood(\n",
    "            means=self.mcmc_means,\n",
    "            covs=self.mcmc_covs,\n",
    "            weights=self.mcmc_weights,\n",
    "        )\n",
    "\n",
    "        # Prior bounds (uniform box)\n",
    "        low_np, high_np = self.make_auto_bounds_inflated(\n",
    "            means=means,\n",
    "            covs=covariances,\n",
    "            inflate=float(self.params.get(\"prior_inflate\", 9.0)),\n",
    "            nsig=float(self.params.get(\"prior_nsig\", 12.0)),\n",
    "            pad=float(self.params.get(\"prior_pad\", 1e-6)),\n",
    "        )\n",
    "        self.prior_low = jnp.asarray(low_np)\n",
    "        self.prior_high = jnp.asarray(high_np)\n",
    "\n",
    "        # Student-t geometry in theta-space (use mixture moments as default)\n",
    "        self.geom_mu, self.geom_cov = mixture_mean_cov(\n",
    "            self.mcmc_means, self.mcmc_covs, self.mcmc_weights, jitter=float(self.params.get(\"geom_jitter\", 1e-6))\n",
    "        )\n",
    "        self.geom_nu = jnp.asarray(self.params.get(\"geom_nu\", 5.0), dtype=self.geom_mu.dtype)\n",
    "\n",
    "        # Convenience target fn (optional)\n",
    "        self.target_fn = self.target_normal\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Public API\n",
    "    # -------------------------------------------------------------------------\n",
    "    def attach_flow_and_scaler(self, *, flow, scaler_cfg, scaler_masks):\n",
    "        \"\"\"Call this if you cannot provide these objects in __init__.\"\"\"\n",
    "        self.flow = flow\n",
    "        self.scaler_cfg = scaler_cfg\n",
    "        self.scaler_masks = scaler_masks\n",
    "\n",
    "    def run_experiment(self):\n",
    "        sampler = self.params.get(\"sampler\", \"precond_pcn\")\n",
    "\n",
    "        if self.params[\"experiment_type\"] == \"gaussian\" and sampler == \"precond_pcn\":\n",
    "            self._run_preconditioned_pcn_gaussian()\n",
    "            return\n",
    "\n",
    "        raise ValueError(\n",
    "            f\"Unsupported combination experiment_type={self.params['experiment_type']} sampler={sampler}\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core run method (your algorithm)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _run_preconditioned_pcn_gaussian(self):\n",
    "        # --- required objects ---\n",
    "        if self.flow is None:\n",
    "            # If you want a hard error instead, replace with raise ValueError(...)\n",
    "            print(\"Warning: self.flow is None; using IdentityFlow() for wiring test.\")\n",
    "            self.flow = IdentityFlow()\n",
    "\n",
    "        if self.scaler_cfg is None or self.scaler_masks is None:\n",
    "            raise ValueError(\n",
    "                \"scaler_cfg / scaler_masks are required for inverse_jax/forward_jax. \"\n",
    "                \"Attach them via attach_flow_and_scaler(...) or pass into __init__.\"\n",
    "            )\n",
    "\n",
    "        D = int(self.params[\"n_dims\"])\n",
    "        N = int(self.params.get(\"n_walkers\", 2048))\n",
    "\n",
    "        # Outer iterations: each call to preconditioned_pcn_jax adapts sigma/mu internally up to n_max\n",
    "        n_outer = int(self.params.get(\"n_outer\", 50))\n",
    "\n",
    "        # Kernel parameters\n",
    "        beta = jnp.asarray(self.params.get(\"beta\", 1.0), dtype=jnp.float32)\n",
    "        n_max = int(self.params.get(\"n_max\", 2000))\n",
    "        n_steps = int(self.params.get(\"n_steps\", 100))\n",
    "        proposal_scale = jnp.asarray(self.params.get(\"proposal_scale\", 0.2), dtype=jnp.float32)\n",
    "\n",
    "        seed = int(self.params.get(\"seed\", 0))\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "\n",
    "        # Prior / likelihood functions in required signatures\n",
    "        logprior_fn = make_uniform_box_logprior(self.prior_low, self.prior_high)\n",
    "        blob0 = jnp.zeros((0,), dtype=jnp.float32)\n",
    "\n",
    "        def loglike_fn(xi):\n",
    "            ll = self.likelihood.log_prob(xi)  # scalar\n",
    "            return ll, blob0\n",
    "\n",
    "        # -----------------------------\n",
    "        # Initialize ensemble state\n",
    "        # -----------------------------\n",
    "        key, k_init = jax.random.split(key, 2)\n",
    "        u = jax.random.normal(k_init, shape=(N, D), dtype=jnp.float32)\n",
    "\n",
    "        x, logdetj = inverse_jax(u, self.scaler_cfg, self.scaler_masks)\n",
    "\n",
    "        # keep boundary-condition roundtrip consistent with your kernel\n",
    "        x_bc = apply_boundary_conditions_x_jax(x, dict(self.scaler_cfg))\n",
    "        u_bc = forward_jax(x_bc, self.scaler_cfg, self.scaler_masks)\n",
    "        x, logdetj = inverse_jax(u_bc, self.scaler_cfg, self.scaler_masks)\n",
    "        u = u_bc\n",
    "\n",
    "        finite0 = jnp.isfinite(logdetj) & jnp.all(jnp.isfinite(x), axis=1)\n",
    "\n",
    "        def _prior_or_neginf(xi, ok):\n",
    "            return jax.lax.cond(\n",
    "                ok,\n",
    "                lambda z: logprior_fn(z),\n",
    "                lambda z: jnp.asarray(-jnp.inf, dtype=xi.dtype),\n",
    "                xi,\n",
    "            )\n",
    "\n",
    "        logp = jax.vmap(_prior_or_neginf, in_axes=(0, 0), out_axes=0)(x, finite0)\n",
    "        finite1 = finite0 & jnp.isfinite(logp)\n",
    "\n",
    "        def _like_or_neginf(xi, ok):\n",
    "            def _do(z):\n",
    "                return loglike_fn(z)\n",
    "            def _skip(z):\n",
    "                return jnp.asarray(-jnp.inf, dtype=xi.dtype), blob0\n",
    "            return jax.lax.cond(ok, _do, _skip, xi)\n",
    "\n",
    "        logl, _ = jax.vmap(_like_or_neginf, in_axes=(0, 0), out_axes=(0, 0))(x, finite1)\n",
    "\n",
    "        # Required by kernel interface; it recomputes logdetj_flow internally anyway\n",
    "        logdetj_flow = jnp.zeros((N,), dtype=jnp.float32)\n",
    "        blobs = jnp.zeros((N, 0), dtype=jnp.float32)\n",
    "\n",
    "        # storage\n",
    "        xs = []\n",
    "        accept_hist = []\n",
    "        sigma_hist = []\n",
    "        calls_hist = []\n",
    "\n",
    "        # -----------------------------\n",
    "        # Outer loop: accumulate samples\n",
    "        # -----------------------------\n",
    "        for t in range(n_outer):\n",
    "            out = preconditioned_pcn_jax(\n",
    "                key,\n",
    "                u=u,\n",
    "                x=x,\n",
    "                logdetj=logdetj,\n",
    "                logl=logl,\n",
    "                logp=logp,\n",
    "                logdetj_flow=logdetj_flow,\n",
    "                blobs=blobs,\n",
    "                beta=beta,\n",
    "                loglike_fn=loglike_fn,\n",
    "                logprior_fn=logprior_fn,\n",
    "                flow=self.flow,\n",
    "                scaler_cfg=self.scaler_cfg,\n",
    "                scaler_masks=self.scaler_masks,\n",
    "                geom_mu=self.geom_mu,\n",
    "                geom_cov=self.geom_cov,\n",
    "                geom_nu=self.geom_nu,\n",
    "                n_max=n_max,\n",
    "                n_steps=n_steps,\n",
    "                proposal_scale=proposal_scale,\n",
    "                condition=None,\n",
    "            )\n",
    "\n",
    "            # update state\n",
    "            key = out[\"key\"]\n",
    "            u = out[\"u\"]\n",
    "            x = out[\"x\"]\n",
    "            logdetj = out[\"logdetj\"]\n",
    "            logdetj_flow = out[\"logdetj_flow\"]\n",
    "            logl = out[\"logl\"]\n",
    "            logp = out[\"logp\"]\n",
    "            blobs = out[\"blobs\"]\n",
    "\n",
    "            xs.append(x)\n",
    "            accept_hist.append(out[\"accept\"])\n",
    "            sigma_hist.append(out[\"proposal_scale\"])\n",
    "            calls_hist.append(out[\"calls\"])\n",
    "\n",
    "            if (t + 1) % int(self.params.get(\"print_every\", 10)) == 0:\n",
    "                acc = float(np.asarray(out[\"accept\"]))\n",
    "                sig = float(np.asarray(out[\"proposal_scale\"]))\n",
    "                calls = int(np.asarray(out[\"calls\"]))\n",
    "                steps = int(np.asarray(out[\"steps\"]))\n",
    "                print(f\"[outer {t+1:>4d}/{n_outer}] accept={acc:.4f} sigma={sig:.4f} calls={calls} steps={steps}\")\n",
    "\n",
    "        # Store results\n",
    "        self.samples = np.asarray(jnp.stack(xs, axis=0))  # (n_outer, N, D)\n",
    "        self.accept_history = np.asarray(jnp.stack(accept_hist))\n",
    "        self.sigma_history = np.asarray(jnp.stack(sigma_hist))\n",
    "        self.calls_history = np.asarray(jnp.stack(calls_hist))\n",
    "\n",
    "        # Convenience summary\n",
    "        print(\n",
    "            f\"Done. samples shape={self.samples.shape} \"\n",
    "            f\"mean_accept={self.accept_history.mean():.4f} \"\n",
    "            f\"last_sigma={self.sigma_history[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Existing utilities / diagnostics\n",
    "    # -------------------------------------------------------------------------\n",
    "    def target_normal(self, x, data=None):\n",
    "        return self.likelihood.log_prob(x)\n",
    "\n",
    "    def get_next_available_outdir(self, base_dir: str, prefix: str = \"results\") -> str:\n",
    "        if not os.path.exists(base_dir):\n",
    "            os.makedirs(base_dir)\n",
    "\n",
    "        existing = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "        matches = [re.match(rf\"{prefix}_(\\d+)\", name) for name in existing]\n",
    "        numbers = [int(m.group(1)) for m in matches if m]\n",
    "        next_number = max(numbers, default=0) + 1\n",
    "        return os.path.join(base_dir, f\"{prefix}_{next_number}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def make_auto_bounds_inflated(means, covs, inflate=9.0, nsig=12.0, pad=1e-6,\n",
    "                                 prior_low=None, prior_high=None):\n",
    "        means = np.asarray(means, dtype=float)                 # (K, D)\n",
    "        covs = np.asarray(covs, dtype=float) * float(inflate)  # inflate variance\n",
    "\n",
    "        mu_min = means.min(axis=0)                             # (D,)\n",
    "        mu_max = means.max(axis=0)                             # (D,)\n",
    "\n",
    "        std_max = np.sqrt(np.stack([np.diag(C) for C in covs], axis=0)).max(axis=0)  # (D,)\n",
    "\n",
    "        low = mu_min - nsig * std_max - pad\n",
    "        high = mu_max + nsig * std_max + pad\n",
    "\n",
    "        if prior_low is not None:\n",
    "            low = np.minimum(low, float(prior_low))\n",
    "        if prior_high is not None:\n",
    "            high = np.maximum(high, float(prior_high))\n",
    "        return low, high\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def get_true_and_mcmc_samples(self, discard=0, thin=1):\n",
    "        dim = int(self.params[\"n_dims\"])\n",
    "\n",
    "        if not hasattr(self, \"true_samples\") or self.true_samples is None:\n",
    "            raise ValueError(\"No true samples found. Ensure self.true_samples is set (gaussian experiment).\")\n",
    "\n",
    "        true_np = np.asarray(self.true_samples).reshape(-1, dim)\n",
    "\n",
    "        if hasattr(self, \"samples\") and self.samples is not None:\n",
    "            samp = np.asarray(self.samples).reshape(-1, dim)  # works for (n_outer, N, D) too\n",
    "            samp = samp[int(discard)::int(thin), :]\n",
    "            mcmc_np = samp\n",
    "        else:\n",
    "            raise ValueError(\"No sampler samples found. Run run_experiment() first.\")\n",
    "\n",
    "        return true_np, mcmc_np\n",
    "    \n",
    "\n",
    "\n",
    "    def plot_true_vs_mcmc_corner(self, seed=2046):\n",
    "        \"\"\"\n",
    "        Overlay corner plot:\n",
    "        - MCMC production samples (black)\n",
    "        - true samples (red)\n",
    "        Saves: true_vs_mcmc_corner_plot.pdf\n",
    "        \"\"\"\n",
    "        # Get samples \n",
    "        true_np, mcmc_np = self.get_true_and_mcmc_samples()\n",
    "\n",
    "        dim = int(self.params[\"n_dims\"])\n",
    "        labels = [f\"x{i}\" for i in range(dim)]\n",
    "\n",
    "        outdir = self.params[\"outdir\"]\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        # Plot MCMC first \n",
    "        fig = corner.corner(\n",
    "            mcmc_np,\n",
    "            color=\"black\",\n",
    "            hist_kwargs={\"color\": \"black\", \"density\": True},\n",
    "            show_titles=True,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        # Overlay true samples \n",
    "        corner.corner(\n",
    "            true_np,\n",
    "            fig=fig,\n",
    "            color=\"red\",\n",
    "            hist_kwargs={\"color\": \"red\", \"density\": True},\n",
    "            show_titles=True,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        # Legend\n",
    "        handles = [\n",
    "            plt.Line2D([], [], color=\"black\", label=\"pocomc\"),\n",
    "            plt.Line2D([], [], color=\"red\", label=\"True Normal\"),\n",
    "        ]\n",
    "        fig.legend(handles=handles, loc=\"upper right\")\n",
    "\n",
    "        save_name = os.path.join(outdir, \"true_vs_mcmc_corner_plot.pdf\")\n",
    "        fig.savefig(save_name, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"Saved overlay corner plot to {save_name}\")\n",
    "\n",
    "\n",
    "\n",
    "    def plot_acceptance_rate(self):\n",
    "        print(\"Plotting acceptance-rate diagnostic curve...\")\n",
    "\n",
    "        if self.accept_history is None:\n",
    "            raise ValueError(\"No accept_history found. Run run_experiment() first.\")\n",
    "\n",
    "        accept = np.asarray(self.accept_history).reshape(-1)\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(accept)\n",
    "        plt.xlabel(\"Outer iteration\")\n",
    "        plt.ylabel(\"Mean acceptance (alpha)\")\n",
    "        plt.title(\"Preconditioned pCN Acceptance\")\n",
    "        save_name = os.path.join(self.params[\"outdir\"], \"acceptance_rate_curve.pdf\")\n",
    "        plt.savefig(save_name, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"Saved to {save_name}\")\n",
    "\n",
    "\n",
    "\n",
    "    def plot_sigma(self):\n",
    "        if self.sigma_history is None:\n",
    "            raise ValueError(\"No sigma_history found. Run run_experiment() first.\")\n",
    "\n",
    "        sig = np.asarray(self.sigma_history).reshape(-1)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(sig)\n",
    "        plt.xlabel(\"Outer iteration\")\n",
    "        plt.ylabel(\"proposal_scale (sigma)\")\n",
    "        plt.title(\"Preconditioned pCN Sigma Adaptation\")\n",
    "        save_name = os.path.join(self.params[\"outdir\"], \"sigma_curve.pdf\")\n",
    "        plt.savefig(save_name, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"Saved to {save_name}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    # 3.3. SAMPLE STATISTICS\n",
    "    #-----------------------------------------------------------------------------\n",
    "    def save_samples_json(self):\n",
    "        # output directory \n",
    "        outdir = self.params[\"outdir\"]\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        # get samples once\n",
    "        true_np, mcmc_np = self.get_true_and_mcmc_samples()\n",
    "\n",
    "        # save generated samples\n",
    "        mcmc_path = os.path.join(outdir, \"mcmc_samples.json\")\n",
    "        with open(mcmc_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(mcmc_np.tolist(), f)\n",
    "        print(f\"MCMC samples saved to {mcmc_path}\")\n",
    "\n",
    "        # save true samples\n",
    "        true_path = os.path.join(outdir, \"true_samples.json\")\n",
    "        with open(true_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(true_np.tolist(), f)\n",
    "        print(f\"True samples saved to {true_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_and_save_sample_statistics(self):\n",
    "        \"\"\"\n",
    "        Computes and saves per-dimension statistics for:\n",
    "        - MCMC production samples\n",
    "        - true samples\n",
    "        Saves: sample_statistics.txt in self.params[\"outdir\"]\n",
    "        \"\"\"\n",
    "\n",
    "        # get samples \n",
    "        true_samples, mcmc_samples = self.get_true_and_mcmc_samples()\n",
    "\n",
    "        # MCMC stats\n",
    "        self.pm = mcmc_samples.mean(axis=0)\n",
    "        self.pv = mcmc_samples.var(axis=0)\n",
    "        self.ps = mcmc_samples.std(axis=0)\n",
    "\n",
    "        # True stats\n",
    "        self.qm = true_samples.mean(axis=0)\n",
    "        self.qv = true_samples.var(axis=0)\n",
    "        self.qs = true_samples.std(axis=0)\n",
    "\n",
    "        # store arrays \n",
    "        self.mcmc_samples = mcmc_samples\n",
    "        self.true_samples_np = true_samples\n",
    "\n",
    "        np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "        stats_str = (\n",
    "            \"pm (mean of MCMC samples):\\n\" + str(self.pm) +\n",
    "            \"\\n\\npv (variance of MCMC samples):\\n\" + str(self.pv) +\n",
    "            \"\\n\\nps (std dev of MCMC samples):\\n\" + str(self.ps) +\n",
    "            \"\\n\\nqm (mean of true samples):\\n\" + str(self.qm) +\n",
    "            \"\\n\\nqv (variance of true samples):\\n\" + str(self.qv) +\n",
    "            \"\\n\\nqs (std dev of true samples):\\n\" + str(self.qs) + \"\\n\"\n",
    "        )\n",
    "\n",
    "        outdir = self.params[\"outdir\"]\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        stats_path = os.path.join(outdir, \"sample_statistics.txt\")\n",
    "        with open(stats_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(stats_str)\n",
    "\n",
    "        print(f\"Sample statistics saved to {stats_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    #-----------------------------------------------------------------------------\n",
    "    # 3.4. KL DIVERGENCE\n",
    "    #-----------------------------------------------------------------------------\n",
    "    import numpy as np, warnings, os\n",
    "    from typing import Tuple\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def gau_kl(pm: np.ndarray, pv: np.ndarray,\n",
    "               qm: np.ndarray, qv: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Kullback-Liebler divergence from Gaussian pm,pv to Gaussian qm,qv.\n",
    "        Also computes KL divergence from a single Gaussian pm,pv to a set\n",
    "         of Gaussians qm,qv.\n",
    "        Diagonal covariances are assumed.  Divergence is expressed in nats.\n",
    "        \"\"\"\n",
    "        if (len(qm.shape) == 2):\n",
    "            axis = 1\n",
    "        else:\n",
    "            axis = 0\n",
    "        # Determinants of diagonal covariances pv, qv\n",
    "        dpv = pv.prod()\n",
    "        dqv = qv.prod(axis)\n",
    "        # Inverse of diagonal covariance qv\n",
    "        iqv = 1. / qv\n",
    "        # Difference between means pm, qm\n",
    "        diff = qm - pm\n",
    "        return (0.5 * (\n",
    "            np.log(dqv / dpv)                 # log |\\Sigma_q| / |\\Sigma_p|\n",
    "            + (iqv * pv).sum(axis)            # + tr(\\Sigma_q^{-1} * \\Sigma_p)\n",
    "            + (diff * iqv * diff).sum(axis)   # + (\\mu_q-\\mu_p)^T\\Sigma_q^{-1}(\\mu_q-\\mu_p)\n",
    "            - len(pm)                         # - N\n",
    "        ))\n",
    "    \n",
    "\n",
    "    def kl_metrics(\n",
    "        self,\n",
    "        outdir: str | None = None,\n",
    "        filename: str = \"kl_metrics.txt\",\n",
    "    ) -> None:\n",
    "        import os\n",
    "        import numpy as np\n",
    "\n",
    "        # define outdir\n",
    "        outdir = (\n",
    "            outdir\n",
    "            or (getattr(self, \"params\", {}) or {}).get(\"outdir\", None)\n",
    "            or getattr(self, \"outdir\", None)\n",
    "        )\n",
    "        if outdir is None:\n",
    "            raise ValueError(\"No output directory specified (pass outdir=... or set params['outdir']).\")\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "        \n",
    "        true_np, mcmc_np = self.get_true_and_mcmc_samples() \n",
    "\n",
    "        # Parametric Gaussian stats (diagonal covariance assumed)\n",
    "        pm = mcmc_np.mean(axis=0)\n",
    "        pv = mcmc_np.var(axis=0)\n",
    "        qm = true_np.mean(axis=0)\n",
    "        qv = true_np.var(axis=0)\n",
    "\n",
    "        kl_val = self.gau_kl(pm, pv, qm, qv)  # scalar for 1D qm/qv\n",
    "\n",
    "        out_path = os.path.join(outdir, filename)\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            if np.isscalar(kl_val):\n",
    "                f.write(f\"Parametric KL (Gaussian): {float(kl_val):.8f}\\n\")\n",
    "            else:\n",
    "                kl_arr = np.asarray(kl_val).ravel()\n",
    "                f.write(\"Parametric KL (Gaussian):\\n\")\n",
    "                for i, v in enumerate(kl_arr):\n",
    "                    f.write(f\"  [{i}] {float(v):.8f}\\n\")\n",
    "\n",
    "        print(f\"KL metrics saved to {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2bfad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "# Identity \"scaler\": u <-> x\n",
    "def inverse_jax(u, scaler_cfg=None, scaler_masks=None):\n",
    "    u = jnp.asarray(u)\n",
    "    logdet = jnp.zeros((u.shape[0],), dtype=u.dtype)  # (N,)\n",
    "    return u, logdet\n",
    "\n",
    "def forward_jax(x, scaler_cfg=None, scaler_masks=None):\n",
    "    return jnp.asarray(x)\n",
    "\n",
    "def apply_boundary_conditions_x_jax(x, cfg_dict=None):\n",
    "    return jnp.asarray(x)\n",
    "\n",
    "flow = IdentityFlow()\n",
    "scaler_cfg = {}      # empty mapping is OK because identity funcs ignore it\n",
    "scaler_masks = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad7198",
   "metadata": {},
   "source": [
    "---\n",
    "# weak\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab890d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    outdir=\"./results\",\n",
    "    experiment_type=\"gaussian\",\n",
    "\n",
    "    n_dims=5,\n",
    "    nr_of_components=4,\n",
    "    nr_of_samples=10000,\n",
    "    width_mean=10.0,\n",
    "    width_cov=1.0,\n",
    "    weights_of_components=None,\n",
    "\n",
    "    sampler=\"precond_pcn\",\n",
    "    n_walkers=4000,        # start smaller for a quick test\n",
    "    n_outer=100,\n",
    "    n_max=9000,\n",
    "    n_steps=300,\n",
    "    proposal_scale=0.2,\n",
    "    beta=1.0,\n",
    "    seed=55,\n",
    "    print_every=10,\n",
    "\n",
    "    geom_nu=1,\n",
    "    prior_inflate=16.0,\n",
    "    prior_nsig=18.0,\n",
    "    prior_pad=1e-6,\n",
    "    geom_jitter=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4109ec76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using output directory: ./results\\results_10\n",
      "Passed parameters:\n",
      "outdir: ./results\\results_10\n",
      "experiment_type: gaussian\n",
      "n_dims: 5\n",
      "nr_of_components: 4\n",
      "nr_of_samples: 10000\n",
      "width_mean: 10.0\n",
      "width_cov: 1.0\n",
      "weights_of_components: [0.25, 0.25, 0.25, 0.25]\n",
      "sampler: precond_pcn\n",
      "n_walkers: 4000\n",
      "n_outer: 100\n",
      "n_max: 9000\n",
      "n_steps: 300\n",
      "proposal_scale: 0.2\n",
      "beta: 1.0\n",
      "seed: 55\n",
      "print_every: 10\n",
      "geom_nu: 1\n",
      "prior_inflate: 16.0\n",
      "prior_nsig: 18.0\n",
      "prior_pad: 1e-06\n",
      "geom_jitter: 1e-06\n",
      "Setting the target function to a Gaussian mixture distribution.\n",
      "[-1.9527316 -5.8704066  7.5672913 -5.8052754 -0.1505208]\n",
      "[-1.7254925   1.308074    2.4973822  -0.90360403 -6.615944  ]\n",
      "[-1.6282320e+00 -1.3995171e-03  3.6579323e+00 -2.1062398e+00\n",
      " -2.6911259e-01]\n",
      "[ 1.3372827 -3.7496185  6.6618586  6.768627   7.777319 ]\n",
      "[outer   10/100] accept=0.2454 sigma=0.1493 calls=36000000 steps=9000\n",
      "[outer   20/100] accept=0.2360 sigma=0.1487 calls=36000000 steps=9000\n",
      "[outer   30/100] accept=0.2429 sigma=0.1471 calls=36000000 steps=9000\n",
      "[outer   40/100] accept=0.2305 sigma=0.1479 calls=36000000 steps=9000\n",
      "[outer   50/100] accept=0.2332 sigma=0.1490 calls=36000000 steps=9000\n",
      "[outer   60/100] accept=0.2388 sigma=0.1490 calls=36000000 steps=9000\n",
      "[outer   70/100] accept=0.2336 sigma=0.1469 calls=36000000 steps=9000\n",
      "[outer   80/100] accept=0.2370 sigma=0.1411 calls=36000000 steps=9000\n",
      "[outer   90/100] accept=0.2306 sigma=0.1429 calls=36000000 steps=9000\n",
      "[outer  100/100] accept=0.2273 sigma=0.1447 calls=36000000 steps=9000\n",
      "Done. samples shape=(100, 4000, 5) mean_accept=0.2343 last_sigma=0.1447\n",
      "Saved overlay corner plot to ./results\\results_10\\true_vs_mcmc_corner_plot.pdf\n",
      "Plotting acceptance-rate diagnostic curve...\n",
      "Saved to ./results\\results_10\\acceptance_rate_curve.pdf\n",
      "Saved to ./results\\results_10\\sigma_curve.pdf\n",
      "MCMC samples saved to ./results\\results_10\\mcmc_samples.json\n",
      "True samples saved to ./results\\results_10\\true_samples.json\n",
      "Sample statistics saved to ./results\\results_10\\sample_statistics.txt\n",
      "KL metrics saved to ./results\\results_10\\kl_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Get the arguments passed over from the command line, and create the experiment runner\n",
    "    \n",
    "    runner = pcn_ExperimentRunner(args, flow=flow, scaler_cfg=scaler_cfg, scaler_masks=scaler_masks)\n",
    "    runner.run_experiment()\n",
    "    runner.plot_true_vs_mcmc_corner()\n",
    "    runner.plot_acceptance_rate()\n",
    "    runner.plot_sigma()\n",
    "    runner.save_samples_json()\n",
    "    runner.compute_and_save_sample_statistics()\n",
    "    runner.kl_metrics()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c57be",
   "metadata": {},
   "source": [
    "---\n",
    "# strong\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    outdir=\"./results\",\n",
    "    experiment_type=\"gaussian\",\n",
    "\n",
    "    # gaussian-mixture generator params (match your parser names)\n",
    "    n_dims=5,\n",
    "    nr_of_components=4,\n",
    "    nr_of_samples=10000,\n",
    "    width_mean=10.0,\n",
    "    width_cov=1.0,\n",
    "    weights_of_components=None,     # or a list like [0.25,0.25,0.25,0.25]\n",
    "\n",
    "    sampler=\"precond_pcn\",\n",
    "\n",
    "    # exploration \n",
    "    n_walkers=4096,     \n",
    "    n_outer=300,        \n",
    "    n_max=15000,        \n",
    "    n_steps=500,        \n",
    "\n",
    "    # proposal / acceptance\n",
    "    proposal_scale=0.2,  \n",
    "    beta=1.0,\n",
    "\n",
    "    # heavier tails in Student-t mixture proposal\n",
    "    geom_nu=2.5,          \n",
    "\n",
    "    # prior box: avoid clipping the target\n",
    "    prior_inflate=16.0,   \n",
    "    prior_nsig=18.0,      \n",
    "    prior_pad=1e-6,\n",
    "\n",
    "    # geometry jitter\n",
    "    geom_jitter=1e-6,\n",
    "\n",
    "    seed=0,\n",
    "    print_every=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4078ff9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using output directory: ./results\\results_8\n",
      "Passed parameters:\n",
      "outdir: ./results\\results_8\n",
      "experiment_type: gaussian\n",
      "n_dims: 5\n",
      "nr_of_components: 4\n",
      "nr_of_samples: 10000\n",
      "width_mean: 10.0\n",
      "width_cov: 1.0\n",
      "weights_of_components: None\n",
      "sampler: precond_pcn\n",
      "n_walkers: 4096\n",
      "n_outer: 300\n",
      "n_max: 15000\n",
      "n_steps: 500\n",
      "proposal_scale: 0.45\n",
      "beta: 1.0\n",
      "geom_nu: 2.5\n",
      "prior_inflate: 16.0\n",
      "prior_nsig: 18.0\n",
      "prior_pad: 1e-06\n",
      "geom_jitter: 1e-06\n",
      "seed: 0\n",
      "print_every: 10\n",
      "Setting the target function to a Gaussian mixture distribution.\n",
      "[-1.9527316 -5.8704066  7.5672913 -5.8052754 -0.1505208]\n",
      "[-1.7254925   1.308074    2.4973822  -0.90360403 -6.615944  ]\n",
      "[-1.6282320e+00 -1.3995171e-03  3.6579323e+00 -2.1062398e+00\n",
      " -2.6911259e-01]\n",
      "[ 1.3372827 -3.7496185  6.6618586  6.768627   7.777319 ]\n",
      "[outer   10/300] accept=0.2232 sigma=0.1429 calls=61440000 steps=15000\n",
      "[outer   20/300] accept=0.2282 sigma=0.1388 calls=61440000 steps=15000\n",
      "[outer   30/300] accept=0.2274 sigma=0.1419 calls=61440000 steps=15000\n",
      "[outer   40/300] accept=0.2282 sigma=0.1395 calls=61440000 steps=15000\n",
      "[outer   50/300] accept=0.2354 sigma=0.1416 calls=61440000 steps=15000\n",
      "[outer   60/300] accept=0.2392 sigma=0.1391 calls=61440000 steps=15000\n",
      "[outer   70/300] accept=0.2296 sigma=0.1403 calls=61440000 steps=15000\n",
      "[outer   80/300] accept=0.2336 sigma=0.1402 calls=61440000 steps=15000\n",
      "[outer   90/300] accept=0.2289 sigma=0.1431 calls=61440000 steps=15000\n",
      "[outer  100/300] accept=0.2442 sigma=0.1387 calls=61440000 steps=15000\n",
      "[outer  110/300] accept=0.2209 sigma=0.1383 calls=61440000 steps=15000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     runner.kl_metrics()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Get the arguments passed over from the command line, and create the experiment runner\u001b[39;00m\n\u001b[32m      4\u001b[39m     runner = pcn_ExperimentRunner(args, flow=flow, scaler_cfg=scaler_cfg, scaler_masks=scaler_masks)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     runner.plot_true_vs_mcmc_corner()\n\u001b[32m      7\u001b[39m     runner.plot_acceptance_rate()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 197\u001b[39m, in \u001b[36mpcn_ExperimentRunner.run_experiment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    194\u001b[39m sampler = \u001b[38;5;28mself\u001b[39m.params.get(\u001b[33m\"\u001b[39m\u001b[33msampler\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprecond_pcn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.params[\u001b[33m\"\u001b[39m\u001b[33mexperiment_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mgaussian\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m sampler == \u001b[33m\"\u001b[39m\u001b[33mprecond_pcn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_preconditioned_pcn_gaussian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    201\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported combination experiment_type=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.params[\u001b[33m'\u001b[39m\u001b[33mexperiment_type\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sampler=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msampler\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 293\u001b[39m, in \u001b[36mpcn_ExperimentRunner._run_preconditioned_pcn_gaussian\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# Outer loop: accumulate samples\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_outer):\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     out = \u001b[43mpreconditioned_pcn_jax\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mu\u001b[49m\u001b[43m=\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogdetj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogdetj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogdetj_flow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogdetj_flow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloglike_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloglike_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprior_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprior_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mflow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaler_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscaler_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeom_mu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeom_mu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeom_cov\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeom_cov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeom_nu\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgeom_nu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproposal_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproposal_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# update state\u001b[39;00m\n\u001b[32m    318\u001b[39m     key = out[\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mpreconditioned_pcn_jax\u001b[39m\u001b[34m(key, u, x, logdetj, logl, logp, logdetj_flow, blobs, beta, loglike_fn, logprior_fn, flow, scaler_cfg, scaler_masks, geom_mu, geom_cov, geom_nu, n_max, n_steps, proposal_scale, condition)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# initial mean and counter and objective\u001b[39;00m\n\u001b[32m    106\u001b[39m mu = geom_mu\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m sigma0 = \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mminimum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposal_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m logp2_best = jnp.mean(logl + logp)\n\u001b[32m    109\u001b[39m cnt0 = jnp.asarray(\u001b[32m0\u001b[39m, dtype=jnp.int32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\oleks\\miniconda3\\envs\\GRASP\\Lib\\site-packages\\jax\\_src\\numpy\\ufunc_api.py:182\u001b[39m, in \u001b[36mufunc.__call__\u001b[39m\u001b[34m(self, out, where, *args)\u001b[39m\n\u001b[32m    180\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    181\u001b[39m call = \u001b[38;5;28mself\u001b[39m.__static_props[\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_vectorized\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Get the arguments passed over from the command line, and create the experiment runner\n",
    "    \n",
    "    runner = pcn_ExperimentRunner(args, flow=flow, scaler_cfg=scaler_cfg, scaler_masks=scaler_masks)\n",
    "    runner.run_experiment()\n",
    "    runner.plot_true_vs_mcmc_corner()\n",
    "    runner.plot_acceptance_rate()\n",
    "    runner.plot_sigma()\n",
    "    runner.save_samples_json()\n",
    "    runner.compute_and_save_sample_statistics()\n",
    "    runner.kl_metrics()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d48b7",
   "metadata": {},
   "source": [
    "# weak thing\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    outdir=\"./results\",\n",
    "    experiment_type=\"gaussian\",\n",
    "\n",
    "    n_dims=2,\n",
    "    nr_of_components=4,\n",
    "    nr_of_samples=20000,\n",
    "    width_mean=5.0,\n",
    "    width_cov=1.0,\n",
    "    weights_of_components=None,\n",
    "\n",
    "    sampler=\"precond_pcn\",\n",
    "    n_walkers=512,        # start smaller for a quick test\n",
    "    n_outer=20,\n",
    "    n_max=500,\n",
    "    n_steps=50,\n",
    "    proposal_scale=0.2,\n",
    "    beta=1.0,\n",
    "    seed=0,\n",
    "    print_every=5,\n",
    "\n",
    "    geom_nu=5.0,\n",
    "    prior_inflate=9.0,\n",
    "    prior_nsig=12.0,\n",
    "    prior_pad=1e-6,\n",
    "    geom_jitter=1e-6,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRASP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
