{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d44c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d33616",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# EXPERIMENT RUNNER (UPDATED FOR preconditioned_pcn_jax)\n",
    "##################################################################################\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# You likely already have these in your project; keep your existing imports.\n",
    "# -----------------------------------------------------------------------------\n",
    "import GaussianMixtureGenerator \n",
    "import GaussianMixtureLikelihood\n",
    "# from your_project.transforms import inverse_jax, forward_jax, apply_boundary_conditions_x_jax\n",
    "# from your_project.samplers import preconditioned_pcn_jax\n",
    "\n",
    "\n",
    "SUPPORTED_EXPERIMENTS = {\"gaussian\"}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -----------------------------------------------------------------------------\n",
    "def mixture_mean_cov(means: jnp.ndarray, covs: jnp.ndarray, weights: jnp.ndarray, jitter: float = 1e-6):\n",
    "    \"\"\"\n",
    "    Compute mixture mean/cov for a Gaussian mixture:\n",
    "      means:   (K, D)\n",
    "      covs:    (K, D, D)\n",
    "      weights: (K,)\n",
    "    Returns:\n",
    "      mu:  (D,)\n",
    "      cov: (D, D)\n",
    "    \"\"\"\n",
    "    w = weights / jnp.sum(weights)\n",
    "    mu = jnp.sum(w[:, None] * means, axis=0)  # (D,)\n",
    "\n",
    "    diff = means - mu[None, :]  # (K, D)\n",
    "    outer = diff[:, :, None] * diff[:, None, :]  # (K, D, D)\n",
    "\n",
    "    cov = jnp.sum(w[:, None, None] * (covs + outer), axis=0)\n",
    "    cov = cov + jitter * jnp.eye(cov.shape[0], dtype=cov.dtype)\n",
    "    return mu, cov\n",
    "\n",
    "\n",
    "def make_uniform_box_logprior(low: jnp.ndarray, high: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    Uniform prior on a hyper-rectangle:\n",
    "      log p(x) = 0 inside [low, high], -inf outside\n",
    "    \"\"\"\n",
    "    low = jnp.asarray(low)\n",
    "    high = jnp.asarray(high)\n",
    "\n",
    "    def logprior_fn(x: jnp.ndarray) -> jnp.ndarray:\n",
    "        inside = jnp.all((x >= low) & (x <= high))\n",
    "        return jax.lax.select(\n",
    "            inside,\n",
    "            jnp.asarray(0.0, dtype=x.dtype),\n",
    "            jnp.asarray(-jnp.inf, dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "    return logprior_fn\n",
    "\n",
    "\n",
    "class IdentityBijection:\n",
    "    def transform_and_log_det(self, u, condition=None):\n",
    "        return u, jnp.asarray(0.0, dtype=u.dtype)\n",
    "\n",
    "    def inverse_and_log_det(self, theta, condition=None):\n",
    "        return theta, jnp.asarray(0.0, dtype=theta.dtype)\n",
    "\n",
    "\n",
    "class IdentityFlow:\n",
    "    \"\"\"Use this only to validate wiring when you don't have a trained FlowJAX flow yet.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.bijection = IdentityBijection()\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "# Runner\n",
    "##################################################################################\n",
    "class pcn_ExperimentRunner:\n",
    "    \"\"\"\n",
    "    Runner that can generate samples using preconditioned_pcn_jax.\n",
    "\n",
    "    Key design decisions:\n",
    "      - We run the kernel multiple times (n_outer), each time updating the full ensemble state.\n",
    "      - We store x across outer iterations -> samples shape (n_outer, N_walkers, D).\n",
    "        Your diagnostics can flatten via reshape(-1, D).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, *, flow=None, scaler_cfg=None, scaler_masks=None):\n",
    "        self.params = vars(args)\n",
    "\n",
    "        # --- unique outdir ---\n",
    "        base_results_dir = self.params[\"outdir\"]\n",
    "        unique_outdir = self.get_next_available_outdir(base_results_dir)\n",
    "        print(f\"Using output directory: {unique_outdir}\")\n",
    "        os.makedirs(unique_outdir, exist_ok=False)\n",
    "        self.params[\"outdir\"] = unique_outdir\n",
    "\n",
    "        # --- validate experiment ---\n",
    "        if self.params[\"experiment_type\"] not in SUPPORTED_EXPERIMENTS:\n",
    "            raise ValueError(\n",
    "                f\"Experiment type {self.params['experiment_type']} is not supported. \"\n",
    "                f\"Supported types are: {SUPPORTED_EXPERIMENTS}\"\n",
    "            )\n",
    "\n",
    "        print(\"Passed parameters:\")\n",
    "        for k, v in self.params.items():\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "        # Attach (or later set) flow + scaler\n",
    "        self.flow = flow\n",
    "        self.scaler_cfg = scaler_cfg\n",
    "        self.scaler_masks = scaler_masks\n",
    "\n",
    "        # Setup experiment\n",
    "        if self.params[\"experiment_type\"] == \"gaussian\":\n",
    "            self._setup_gaussian_experiment(args)\n",
    "\n",
    "        # Placeholder for results\n",
    "        self.samples = None\n",
    "        self.accept_history = None\n",
    "        self.sigma_history = None\n",
    "        self.calls_history = None\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Setup\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _setup_gaussian_experiment(self, args):\n",
    "        print(\"Setting the target function to a Gaussian mixture distribution.\")\n",
    "\n",
    "        np.random.seed(900)\n",
    "\n",
    "        D = int(self.params[\"n_dims\"])\n",
    "\n",
    "        # Generate \"true\" samples and mixture parameters (your existing generator)\n",
    "        true_samples, means, covariances, weights = GaussianMixtureGenerator.generate_gaussian_mixture(\n",
    "            n_dim=D,\n",
    "            n_gaussians=args.nr_of_components,\n",
    "            n_samples=args.nr_of_samples,\n",
    "            width_mean=args.width_mean,\n",
    "            width_cov=args.width_cov,\n",
    "            weights=args.weights_of_components,\n",
    "        )\n",
    "\n",
    "        self.true_samples = true_samples\n",
    "\n",
    "        # Convert mixture params to JAX arrays\n",
    "        self.mcmc_means = jnp.stack(means, axis=0)        # (K, D)\n",
    "        self.mcmc_covs = jnp.stack(covariances, axis=0)   # (K, D, D)\n",
    "        self.mcmc_weights = jnp.asarray(weights)          # (K,)\n",
    "\n",
    "        # Likelihood object you already have\n",
    "        self.likelihood = GaussianMixtureLikelihood(\n",
    "            means=self.mcmc_means,\n",
    "            covs=self.mcmc_covs,\n",
    "            weights=self.mcmc_weights,\n",
    "        )\n",
    "\n",
    "        # Prior bounds (uniform box)\n",
    "        low_np, high_np = self.make_auto_bounds_inflated(\n",
    "            means=means,\n",
    "            covs=covariances,\n",
    "            inflate=float(self.params.get(\"prior_inflate\", 9.0)),\n",
    "            nsig=float(self.params.get(\"prior_nsig\", 12.0)),\n",
    "            pad=float(self.params.get(\"prior_pad\", 1e-6)),\n",
    "        )\n",
    "        self.prior_low = jnp.asarray(low_np)\n",
    "        self.prior_high = jnp.asarray(high_np)\n",
    "\n",
    "        # Student-t geometry in theta-space (use mixture moments as default)\n",
    "        self.geom_mu, self.geom_cov = mixture_mean_cov(\n",
    "            self.mcmc_means, self.mcmc_covs, self.mcmc_weights, jitter=float(self.params.get(\"geom_jitter\", 1e-6))\n",
    "        )\n",
    "        self.geom_nu = jnp.asarray(self.params.get(\"geom_nu\", 5.0), dtype=self.geom_mu.dtype)\n",
    "\n",
    "        # Convenience target fn (optional)\n",
    "        self.target_fn = self.target_normal\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Public API\n",
    "    # -------------------------------------------------------------------------\n",
    "    def attach_flow_and_scaler(self, *, flow, scaler_cfg, scaler_masks):\n",
    "        \"\"\"Call this if you cannot provide these objects in __init__.\"\"\"\n",
    "        self.flow = flow\n",
    "        self.scaler_cfg = scaler_cfg\n",
    "        self.scaler_masks = scaler_masks\n",
    "\n",
    "    def run_experiment(self):\n",
    "        sampler = self.params.get(\"sampler\", \"precond_pcn\")\n",
    "\n",
    "        if self.params[\"experiment_type\"] == \"gaussian\" and sampler == \"precond_pcn\":\n",
    "            self._run_preconditioned_pcn_gaussian()\n",
    "            return\n",
    "\n",
    "        raise ValueError(\n",
    "            f\"Unsupported combination experiment_type={self.params['experiment_type']} sampler={sampler}\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Core run method (your algorithm)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _run_preconditioned_pcn_gaussian(self):\n",
    "        # --- required objects ---\n",
    "        if self.flow is None:\n",
    "            # If you want a hard error instead, replace with raise ValueError(...)\n",
    "            print(\"Warning: self.flow is None; using IdentityFlow() for wiring test.\")\n",
    "            self.flow = IdentityFlow()\n",
    "\n",
    "        if self.scaler_cfg is None or self.scaler_masks is None:\n",
    "            raise ValueError(\n",
    "                \"scaler_cfg / scaler_masks are required for inverse_jax/forward_jax. \"\n",
    "                \"Attach them via attach_flow_and_scaler(...) or pass into __init__.\"\n",
    "            )\n",
    "\n",
    "        D = int(self.params[\"n_dims\"])\n",
    "        N = int(self.params.get(\"n_walkers\", 2048))\n",
    "\n",
    "        # Outer iterations: each call to preconditioned_pcn_jax adapts sigma/mu internally up to n_max\n",
    "        n_outer = int(self.params.get(\"n_outer\", 50))\n",
    "\n",
    "        # Kernel parameters\n",
    "        beta = jnp.asarray(self.params.get(\"beta\", 1.0), dtype=jnp.float32)\n",
    "        n_max = int(self.params.get(\"n_max\", 2000))\n",
    "        n_steps = int(self.params.get(\"n_steps\", 100))\n",
    "        proposal_scale = jnp.asarray(self.params.get(\"proposal_scale\", 0.2), dtype=jnp.float32)\n",
    "\n",
    "        seed = int(self.params.get(\"seed\", 0))\n",
    "        key = jax.random.PRNGKey(seed)\n",
    "\n",
    "        # Prior / likelihood functions in required signatures\n",
    "        logprior_fn = make_uniform_box_logprior(self.prior_low, self.prior_high)\n",
    "        blob0 = jnp.zeros((0,), dtype=jnp.float32)\n",
    "\n",
    "        def loglike_fn(xi):\n",
    "            ll = self.likelihood.log_prob(xi)  # scalar\n",
    "            return ll, blob0\n",
    "\n",
    "        # -----------------------------\n",
    "        # Initialize ensemble state\n",
    "        # -----------------------------\n",
    "        key, k_init = jax.random.split(key, 2)\n",
    "        u = jax.random.normal(k_init, shape=(N, D), dtype=jnp.float32)\n",
    "\n",
    "        x, logdetj = inverse_jax(u, self.scaler_cfg, self.scaler_masks)\n",
    "\n",
    "        # keep boundary-condition roundtrip consistent with your kernel\n",
    "        x_bc = apply_boundary_conditions_x_jax(x, dict(self.scaler_cfg))\n",
    "        u_bc = forward_jax(x_bc, self.scaler_cfg, self.scaler_masks)\n",
    "        x, logdetj = inverse_jax(u_bc, self.scaler_cfg, self.scaler_masks)\n",
    "        u = u_bc\n",
    "\n",
    "        finite0 = jnp.isfinite(logdetj) & jnp.all(jnp.isfinite(x), axis=1)\n",
    "\n",
    "        def _prior_or_neginf(xi, ok):\n",
    "            return jax.lax.cond(\n",
    "                ok,\n",
    "                lambda z: logprior_fn(z),\n",
    "                lambda z: jnp.asarray(-jnp.inf, dtype=xi.dtype),\n",
    "                xi,\n",
    "            )\n",
    "\n",
    "        logp = jax.vmap(_prior_or_neginf, in_axes=(0, 0), out_axes=0)(x, finite0)\n",
    "        finite1 = finite0 & jnp.isfinite(logp)\n",
    "\n",
    "        def _like_or_neginf(xi, ok):\n",
    "            def _do(z):\n",
    "                return loglike_fn(z)\n",
    "            def _skip(z):\n",
    "                return jnp.asarray(-jnp.inf, dtype=xi.dtype), blob0\n",
    "            return jax.lax.cond(ok, _do, _skip, xi)\n",
    "\n",
    "        logl, _ = jax.vmap(_like_or_neginf, in_axes=(0, 0), out_axes=(0, 0))(x, finite1)\n",
    "\n",
    "        # Required by kernel interface; it recomputes logdetj_flow internally anyway\n",
    "        logdetj_flow = jnp.zeros((N,), dtype=jnp.float32)\n",
    "        blobs = jnp.zeros((N, 0), dtype=jnp.float32)\n",
    "\n",
    "        # storage\n",
    "        xs = []\n",
    "        accept_hist = []\n",
    "        sigma_hist = []\n",
    "        calls_hist = []\n",
    "\n",
    "        # -----------------------------\n",
    "        # Outer loop: accumulate samples\n",
    "        # -----------------------------\n",
    "        for t in range(n_outer):\n",
    "            out = preconditioned_pcn_jax(\n",
    "                key,\n",
    "                u=u,\n",
    "                x=x,\n",
    "                logdetj=logdetj,\n",
    "                logl=logl,\n",
    "                logp=logp,\n",
    "                logdetj_flow=logdetj_flow,\n",
    "                blobs=blobs,\n",
    "                beta=beta,\n",
    "                loglike_fn=loglike_fn,\n",
    "                logprior_fn=logprior_fn,\n",
    "                flow=self.flow,\n",
    "                scaler_cfg=self.scaler_cfg,\n",
    "                scaler_masks=self.scaler_masks,\n",
    "                geom_mu=self.geom_mu,\n",
    "                geom_cov=self.geom_cov,\n",
    "                geom_nu=self.geom_nu,\n",
    "                n_max=n_max,\n",
    "                n_steps=n_steps,\n",
    "                proposal_scale=proposal_scale,\n",
    "                condition=None,\n",
    "            )\n",
    "\n",
    "            # update state\n",
    "            key = out[\"key\"]\n",
    "            u = out[\"u\"]\n",
    "            x = out[\"x\"]\n",
    "            logdetj = out[\"logdetj\"]\n",
    "            logdetj_flow = out[\"logdetj_flow\"]\n",
    "            logl = out[\"logl\"]\n",
    "            logp = out[\"logp\"]\n",
    "            blobs = out[\"blobs\"]\n",
    "\n",
    "            xs.append(x)\n",
    "            accept_hist.append(out[\"accept\"])\n",
    "            sigma_hist.append(out[\"proposal_scale\"])\n",
    "            calls_hist.append(out[\"calls\"])\n",
    "\n",
    "            if (t + 1) % int(self.params.get(\"print_every\", 10)) == 0:\n",
    "                acc = float(np.asarray(out[\"accept\"]))\n",
    "                sig = float(np.asarray(out[\"proposal_scale\"]))\n",
    "                calls = int(np.asarray(out[\"calls\"]))\n",
    "                steps = int(np.asarray(out[\"steps\"]))\n",
    "                print(f\"[outer {t+1:>4d}/{n_outer}] accept={acc:.4f} sigma={sig:.4f} calls={calls} steps={steps}\")\n",
    "\n",
    "        # Store results\n",
    "        self.samples = np.asarray(jnp.stack(xs, axis=0))  # (n_outer, N, D)\n",
    "        self.accept_history = np.asarray(jnp.stack(accept_hist))\n",
    "        self.sigma_history = np.asarray(jnp.stack(sigma_hist))\n",
    "        self.calls_history = np.asarray(jnp.stack(calls_hist))\n",
    "\n",
    "        # Convenience summary\n",
    "        print(\n",
    "            f\"Done. samples shape={self.samples.shape} \"\n",
    "            f\"mean_accept={self.accept_history.mean():.4f} \"\n",
    "            f\"last_sigma={self.sigma_history[-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Existing utilities / diagnostics\n",
    "    # -------------------------------------------------------------------------\n",
    "    def target_normal(self, x, data=None):\n",
    "        return self.likelihood.log_prob(x)\n",
    "\n",
    "    def get_next_available_outdir(self, base_dir: str, prefix: str = \"results\") -> str:\n",
    "        if not os.path.exists(base_dir):\n",
    "            os.makedirs(base_dir)\n",
    "\n",
    "        existing = [d for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "        matches = [re.match(rf\"{prefix}_(\\d+)\", name) for name in existing]\n",
    "        numbers = [int(m.group(1)) for m in matches if m]\n",
    "        next_number = max(numbers, default=0) + 1\n",
    "        return os.path.join(base_dir, f\"{prefix}_{next_number}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def make_auto_bounds_inflated(means, covs, inflate=9.0, nsig=12.0, pad=1e-6,\n",
    "                                 prior_low=None, prior_high=None):\n",
    "        means = np.asarray(means, dtype=float)                 # (K, D)\n",
    "        covs = np.asarray(covs, dtype=float) * float(inflate)  # inflate variance\n",
    "\n",
    "        mu_min = means.min(axis=0)                             # (D,)\n",
    "        mu_max = means.max(axis=0)                             # (D,)\n",
    "\n",
    "        std_max = np.sqrt(np.stack([np.diag(C) for C in covs], axis=0)).max(axis=0)  # (D,)\n",
    "\n",
    "        low = mu_min - nsig * std_max - pad\n",
    "        high = mu_max + nsig * std_max + pad\n",
    "\n",
    "        if prior_low is not None:\n",
    "            low = np.minimum(low, float(prior_low))\n",
    "        if prior_high is not None:\n",
    "            high = np.maximum(high, float(prior_high))\n",
    "        return low, high\n",
    "\n",
    "    def get_true_and_mcmc_samples(self, discard=0, thin=1):\n",
    "        dim = int(self.params[\"n_dims\"])\n",
    "\n",
    "        if not hasattr(self, \"true_samples\") or self.true_samples is None:\n",
    "            raise ValueError(\"No true samples found. Ensure self.true_samples is set (gaussian experiment).\")\n",
    "\n",
    "        true_np = np.asarray(self.true_samples).reshape(-1, dim)\n",
    "\n",
    "        if hasattr(self, \"samples\") and self.samples is not None:\n",
    "            samp = np.asarray(self.samples).reshape(-1, dim)  # works for (n_outer, N, D) too\n",
    "            samp = samp[int(discard)::int(thin), :]\n",
    "            mcmc_np = samp\n",
    "        else:\n",
    "            raise ValueError(\"No sampler samples found. Run run_experiment() first.\")\n",
    "\n",
    "        return true_np, mcmc_np"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GRASP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
